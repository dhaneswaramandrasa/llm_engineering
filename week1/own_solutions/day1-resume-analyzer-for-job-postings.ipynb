{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6700cb-a0b0-4ac2-8fd5-363729284173",
   "metadata": {},
   "source": [
    "# AI-Powered Resume Analyzer for Job Postings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa4891-b283-44de-aa63-f017eb9b140d",
   "metadata": {},
   "source": [
    "This tool is designed to analyze resumes against specific job postings, offering valuable insights such as:\n",
    "\n",
    "- Identification of skill gaps\n",
    "- Keyword matching between the CV and the job description\n",
    "- Tailored recommendations for CV improvement\n",
    "- An alignment score reflecting how well the CV fits the job\n",
    "- Personalized feedback \n",
    "- Job market trend insights\n",
    "\n",
    "An example of the tool's output can be found [here](https://tvarol.github.io/sideProjects/AILLMAgents/output.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a6a34ea-191f-4c54-9793-a3eb63faab23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import requests\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "from pdf2image import convert_from_bytes\n",
    "import pytesseract\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from ipywidgets import Textarea, FileUpload, Button, VBox, HTML\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04bbe1d3-bacc-400c-aed2-db44699e38f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "if not api_key:\n",
    "    print(\"No API key was found!!!\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27bfcee1-58e6-4ff2-9f12-9dc5c1aa5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e79f2-3139-4520-ac01-a728c11cb8b9",
   "metadata": {},
   "source": [
    "## Using a Frontier Model GPT-4o Mini for This Project\n",
    "\n",
    "### Types of Prompts\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0da158ad-c3a8-4cef-806f-be0f90852996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt \n",
    "system_prompt = \"\"\"You are a powerful AI model designed to assist with resume analysis. Your task is to analyze a resume against a given job posting and provide feedback on how well the resume aligns with the job requirements. Your response should include the following: \n",
    "1) Skill gap identification: Compare the skills listed in the resume with those required in the job posting, highlighting areas where the resume may be lacking or overemphasized.\n",
    "2) Keyword matching between a CV and a job posting: Match keywords from the job description with the resume, determining how well they align. Provide specific suggestions for missing keywords to add to the CV.\n",
    "3) Recommendations for CV improvement: Provide actionable suggestions on how to enhance the resume, such as adding missing skills or rephrasing experience to match job requirements.\n",
    "4) Alignment score: Display a score that represents the degree of alignment between the resume and the job posting.\n",
    "5) Personalized feedback: Offer tailored advice based on the job posting, guiding the user on how to optimize their CV for the best chances of success.\n",
    "6) Job market trend insights, provide broader market trends and insights, such as in-demand skills and salary ranges.\n",
    "Provide responses that are concise, clear, and to the point. Respond in markdown.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a3f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_text_from_pdf(content):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(content) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\" if page.extract_text() else \"\"\n",
    "    return text\n",
    "\n",
    "def extract_text_with_ocr(pdf_path):\n",
    "    images = convert_from_path(pdf_path)  # Convert PDF pages to images\n",
    "    text = \"\\n\".join(pytesseract.image_to_string(img) for img in images)\n",
    "    return text.strip()\n",
    "\n",
    "# Call the function with the correct argument\n",
    "extracted_text = extract_text_with_ocr(\"resume_dhaneswara_mandrasa.pdf\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d49c7a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2812237b8a429d9635ff55fe0b5366",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Input Job Posting and CV</h3>'), VBox(children=(Textarea(value='', description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "job_posting_area = Textarea(\n",
    "    placeholder='Paste the job posting text here...',\n",
    "    description='Job Posting:',\n",
    "    disabled=False,\n",
    "    layout={'width': '800px', 'height': '300px'}\n",
    ")\n",
    "\n",
    "# Define file upload for CV\n",
    "cv_upload = FileUpload(\n",
    "    accept='.pdf',  # Only accept PDF files\n",
    "    multiple=False,  # Only allow single file selection\n",
    "    description='Upload CV (PDF)'\n",
    ")\n",
    "\n",
    "status = HTML(value=\"<b>Status:</b> Waiting for inputs...\")\n",
    "\n",
    "# Create Submit Buttons\n",
    "submit_cv_button = Button(description='Submit CV', button_style='success')\n",
    "submit_job_posting_button = Button(description='Submit Job Posting', button_style='success')\n",
    "\n",
    "# Initialize variables to store the data\n",
    "# This dictionary will hold the text for both the job posting and the CV\n",
    "# It will be used to define the user_prompt\n",
    "for_user_prompt = {\n",
    "    'job_posting': '',\n",
    "    'cv_text': ''\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(content):\n",
    "    \"\"\"Extract text from a PDF using pdfplumber.\"\"\"\n",
    "    text = []\n",
    "    with pdfplumber.open(content) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            extracted_text = page.extract_text()\n",
    "            if extracted_text:\n",
    "                text.append(extracted_text)\n",
    "    \n",
    "    return \"\\n\".join(text).strip()\n",
    "\n",
    "def extract_text_with_ocr(content):\n",
    "    \"\"\"Extract text from an image-based PDF using OCR (pytesseract).\"\"\"\n",
    "    images = convert_from_bytes(content.read())  # Convert PDF bytes to images\n",
    "    text = \"\\n\".join(pytesseract.image_to_string(img) for img in images)\n",
    "    return text.strip()\n",
    "\n",
    "def submit_cv_action(change):\n",
    "    \"\"\"Handles CV upload, extracts text, and stores it in for_user_prompt['cv_text'].\"\"\"\n",
    "    if not cv_upload.value:\n",
    "        print(\"DEBUG: No file detected in cv_upload.value\")\n",
    "        status.value = \"<b>Status:</b> No file uploaded. Please upload a CV.\"\n",
    "        return\n",
    "\n",
    "    uploaded_file = cv_upload.value[0]\n",
    "    content = io.BytesIO(uploaded_file['content'])  # Convert to BytesIO\n",
    "\n",
    "    try:\n",
    "        cv_text = extract_text_from_pdf(content)\n",
    "        \n",
    "        if not cv_text:\n",
    "            print(\"DEBUG: The PDF might be image-based, using OCR instead.\")\n",
    "            content.seek(0)  # Reset file pointer\n",
    "            cv_text = extract_text_with_ocr(content)\n",
    "\n",
    "        for_user_prompt['cv_text'] = cv_text\n",
    "        status.value = \"<b>Status:</b> CV uploaded and processed successfully!\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"DEBUG: Error occurred - {str(e)}\")\n",
    "        status.value = f\"<b>Status:</b> Error processing PDF: {str(e)}\"\n",
    "            \n",
    "def submit_job_posting_action(b):\n",
    "    for_user_prompt['job_posting'] = job_posting_area.value\n",
    "    if for_user_prompt['job_posting']:\n",
    "        #print(\"Job Posting Submitted:\")\n",
    "        #print(for_user_prompt['job_posting'])\n",
    "        status.value = \"<b>Status:</b> Job posting submitted successfully!\"\n",
    "    else:\n",
    "        status.value = \"<b>Status:</b> Please enter a job posting before submitting.\"\n",
    "\n",
    "# Attach actions to buttons\n",
    "submit_cv_button.on_click(submit_cv_action)\n",
    "submit_job_posting_button.on_click(submit_job_posting_action)\n",
    "\n",
    "# Layout\n",
    "job_posting_box = VBox([job_posting_area, submit_job_posting_button])\n",
    "cv_buttons = VBox([submit_cv_button])\n",
    "\n",
    "# Display all widgets\n",
    "display(VBox([\n",
    "    HTML(value=\"<h3>Input Job Posting and CV</h3>\"),\n",
    "    job_posting_box, \n",
    "    cv_upload,\n",
    "    cv_buttons,\n",
    "    status\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "364e42a6-0910-4c7c-8c3c-2ca7d2891cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now define user_prompt using for_user_prompt dictionary\n",
    "# Clearly label each input to differentiate the job posting and CV\n",
    "# The model can parse and analyze each section based on these labels\n",
    "user_prompt = f\"\"\"\n",
    "Job Posting: \n",
    "{for_user_prompt['job_posting']}\n",
    "\n",
    "CV: \n",
    "{for_user_prompt['cv_text']}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448050db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(for_user_prompt['job_posting'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "00ed020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dhaneswara Mandrasa\n",
      "Data Science | Machine Learning & Gen AI Engineer\n",
      "dhaneswara.mandrasa@gmail.com|https://www.linkedin.com/in/dhaneswaramandrasa\n",
      "Jakarta|+6285156073967\n",
      "I turn data into actionable insights by implementing AI/ML systems that solve industry-critical problems—from\n",
      "boosting agricultural yields with predictive analytics to optimizing banking risk models. With 7+ years leading\n",
      "data science teams and solutions for corporate clients and deploying LLM-powered tools, I bridge the gap\n",
      "between technical innovation and business impact.\n",
      "My work isn’t theoretical—it’s battle-tested:\n",
      "1. Built Market Intelligence products that contributed to 70% of company revenue in under 2 years.\n",
      "2. Engineered a credit scoring model with a record-breaking Gini coefficient for telecom giants.\n",
      "3. Implemented LLM-powered analytics systems that automated multilingual categorization and translation.\n",
      "4. Scaled retail outlets from 100 to 200+ across Indonesia using geospatial ML models.\n",
      "Work Experience\n",
      "Lead Data Scientist Jul 2024 - Present\n",
      "iZeno | Jakarta\n",
      "iZeno is a leading technology and analytics consultant is South East Asia. My responsibility is to implement\n",
      "and guide a team of data scientists in delivering end-to-end AI/ML solutions, facilitating knowledge transfer\n",
      "through technical workshops and code reviews.\n",
      "Utilized Dataiku DSS to integrate multiple data sources and create highly efficient data pipelines for\n",
      "advanced analytics.\n",
      "Architected and implemented an advanced LLM-powered news analytics system for Labuan using\n",
      "Snowflake Cortex. Developed robust web scraping solutions for Bloomberg and Malaysian news sources.\n",
      "Engineered multilingual LLM pipelines for automated news categorization and translation. Implemented\n",
      "end-to-end solution for news analysis and knowledge graph construction. Reduced misclassification of\n",
      "financial/political news from to 3%. Mapped 10K+ entities/relationships for real-time geopolitical risk.\n",
      "Spearheaded a cross-functional team in developing an advanced diagnostic root cause analysis system\n",
      "for Sinarmas' plantation production, leveraging Dataiku to optimize agricultural yields and operational\n",
      "efficiency.\n",
      "Designed and implemented a sophisticated recommendation system to enhance customer engagement\n",
      "and product targeting for Bank BRI.\n",
      "Accelerated data-driven decision-making by implementing Text-to-SQL (Snowflake Cortex), reducing\n",
      "query-to-insight time by 30% and enabling non-technical teams to generate complex reports\n",
      "autonomously.\n",
      "Enhanced operational efficiency by unifying unstructured data (emails, documents) with structured\n",
      "financial data via Snowflake Cortex, cutting report generation time from 3 hours to 30 minutes.\n",
      "Tools utilized: Dataiku, Snowflake, Python, SQL, Pandas, scikit-learn, Spark ML, Pyspark\n",
      "Senior Data Scientist Apr 2024 - Sep 2024\n",
      "PT Mehulinta Teknologi Analitik | Jakarta\n",
      "Mehulinta teknologi analitik is a premier data consulting firm specializing in delivering world-class data\n",
      "analytics solutions for enterprises.\n",
      "Developed and deployed machine learning models to predict insurance claims for multiple companies,\n",
      "enhancing their ability to manage risk and allocate resources effectively.\n",
      "Implemented ML models to anticipate customer churn across various products, enabling company to\n",
      "proactively address customer retention.\n",
      "Designed and utilized machine learning algorithms to identify potentially fraudulent insurance claims,\n",
      "improving fraud detection and prevention measures.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, Spark ML, Mlflow, Pyspark, Prefect\n",
      "Senior Data Scientist Apr 2021 - Jul 2024\n",
      "Eureka AI | Abu Dhabi\n",
      "Eureka AI is a Big Data Analytics company that monetizes telecommunication data. As Senior Data\n",
      "Scientist, I was responsible to create end-to-end AI products such as Telco Risk/Credit Score and Market\n",
      "Intelligence.\n",
      "Contributing to development of an innovative Market Intelligence products which are contributing to\n",
      "70% of company’s revenue in less than 2 years.\n",
      "Advanced supervised and unsupervised learning models were conducted to segment more than 100\n",
      "million ride hailing and e-commerce daily active users based on clickstream data into hierarchy level of\n",
      "segmentations across five major telecom companies in Asia.\n",
      "Using PySpark ML, MLflow, and scikit-learn, implemented customized k-means clustering and ensemble\n",
      "machine learning models like Random Forest (RF) and XGBoost. To enhance the transparency and\n",
      "interpretability of these models, both intrinsic and model-agnostic techniques were employed.\n",
      "Utilized Word2Vec in Spark to establish a domain look-alike model by examining URL domain similarities,\n",
      "significantly improving the accuracy and efficiency of application and user behavior analysis.\n",
      "Classified more than 30 persona identifications for over 80 million monthly active telecommunication\n",
      "users using clickstream, movement and telecommunication subscriber data.\n",
      "Utilized similarity models involving geolocation, online behavioral data, and mobility features to provide\n",
      "valuable insights into user segmentation and preferences, aiding in targeted marketing strategies.\n",
      "Demonstrated proficiency in ETL process implementation and constructing a credit score model using\n",
      "telecommunication and clickstream data.\n",
      "Achieved an outstanding Gini coefficient of 0.6, the highest ever recorded in the company's history at the\n",
      "time.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, Spark ML, Mlflow, Pyspark, Databricks, Airflow, GCP\n",
      "Data Scientist Jun 2020 - Apr 2021\n",
      "PT. Bhumi Varta Technology | Jakarta\n",
      "Bhumi Varta Technology are changing the world of location intelligence, business analytics, mapping and\n",
      "geo-fencing marketing in Indonesia. As Data Scientist, I help businesses to make the right decision using\n",
      "integrated GIS technology, big data analytics, and customize application.\n",
      "Developed and executed data science research strategies and methodologies.\n",
      "Constructed machine learning models (utilizing Python, scikit-learn, and pandas) on spatial, POI, mobile,\n",
      "and sales data to assist clients in expanding their business from fewer than 100 outlets to over 200\n",
      "outlets throughout Indonesia.\n",
      "Identified poverty levels from satellite images using convolutional neural network models in PyTorch,\n",
      "enhancing the accuracy of Socio Economic Status modules from 1x1 km to 100x100m.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, AWS, PyTorch, Tensorflow\n",
      "Subsurface Data Analyst Apr 2016 - Apr 2018\n",
      "SKK Migas | Jakarta\n",
      "Constructed initial machine learning model to predict oil and gas resources and reserves for more than\n",
      "200 working areas in Indonesia. Reached error percentage < 10% in the test dataset.\n",
      "Managed and collected the subsurface data such well logging, seismic, production, and drilling data\n",
      "from oil and gas companies.\n",
      "Implemented data migration from companies to SKK Migas and database management software testing\n",
      "for SKK Migas.\n",
      "Tools utilized: Python, SQL\n",
      "Core Skills\n",
      "Large Language Models (LLM),MLOps,Dataiku DSS,Snowflake,Project Management,Consulting,\n",
      "Azure Databricks,Python (Programming Language),SQL,PySpark,Machine Learning,Data Analysis,\n",
      "Analytical Skills,Predictive Modeling,Statistical Modeling,Google Cloud Platform (GCP),\n",
      "R (Programming Language),Apache Oozie,Apache Impala,Python\n",
      "Education\n",
      "Institut Teknologi Bandung Jan 2011 - Jan 2015\n",
      "Bachelor of Science (B.Sc) Geological Engineering\n",
      "GPA: 3.52 out of 4.00 (cum laude)\n",
      "Languages\n",
      "English (PROFESSIONAL_WORKING), Indonesian (NATIVE_OR_BILINGUAL)\n",
      "Awards\n",
      "3rd winner of Joint Convention Balikpapan 2015 Individual Geology Student Sep 2015\n",
      "Competition\n",
      "HAGI-IAGI-IAFMI-IATMI\n",
      "Static reservoir modeling, hydrocarbon volume calculation, and new well proposal of an oil field by using\n",
      "Petrel software\n",
      "Dean's List Jan 2015\n",
      "Institut Teknologi Bandung\n",
      "Certificates\n",
      "Google Analytics For Beginners\n",
      "Google\n",
      "Data Science Academy Full Stack: Data Visualization, Machine Learning and Data Analytics\n",
      "Specialization with R, Python and SQL\n",
      "Algoritma Data Science Education Center\n",
      "The Complete SQL Bootcamp\n",
      "Udemy\n",
      "Udacity Data Scientist Nanodegree\n",
      "Udacity\n",
      "Udacity Deep Learning Nanodegree\n",
      "Udacity\n",
      "Dataiku Advance Designer\n",
      "Dataiku\n"
     ]
    }
   ],
   "source": [
    "print(for_user_prompt['cv_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51dda0-9a0c-48f4-8ec8-dae32c29da24",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da72531d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to understand the job posting for a data scientist. Hmm, the person is asking me about it and needs my help. Let's see. The job posting starts with an exciting part: \"Job posting for Data Scientist position.\" They’ve included a clear call-to-action at the end of the introduction, which probably means we should jump in there.\n",
       "\n",
       "Who are the candidates? Probably high school students because they have listed specific coursework and interests related to data science and programming. I noticed something about specific coursework courses like Descriptive Statistics, Hypothesis Testing, Experimental Design, etc., so I think that's a given for any entry-level position.\n",
       "\n",
       "What do they say about their skills? They mention having strong problem-solving abilities, especially with coding and mathematical topics. That makes sense because data scientists use multiple programming languages and statistical models.\n",
       "\n",
       "What kind of roles will the company be looking for? It seems diverse—government agencies in Japan, healthcare companies globally, and both private and public sectors. So they might want a candidate who's versatile to work across different industries.\n",
       "\n",
       "The responsibilities part says organizing training programs and contributing with new data insights. Hmm, that suggests more of a collaborative or mentoring role rather than just coding tasks. They’re looking for someone who can help drive projects forward by applying their skills.\n",
       "\n",
       "Looking at their goals—developing technical skills and business acumen—this aligns well because data scientists need to both code effectively and understand what they're doing in applications. I guess the key is building those two important areas together.\n",
       "\n",
       "What's expected from them? They need to engage with diverse teams, work on end-to-end problems, explain concepts clearly, and be adaptable and motivated. That sounds like a good mix of technical and soft skills. They also point out looking for enthusiasm, creativity, professionalism, and attention to detail—things that are important in any field.\n",
       "\n",
       "What kind of education do they need? Probably foundational programming (Python, SQL), statistics, machine learning, plus relevant coursework. I think their level shouldn't be too advanced or too basic; it's something just between a beginner and an experienced data professional.\n",
       "</think>\n",
       "\n",
       "The job posting for the role of Data Scientist at various companies includes specific educational requirements. The key points are:\n",
       "\n",
       "1. **Education Requirements**: Entry-level candidates should have foundational programming skills in Python, SQL, as well as introductory statistics, machine learning, and courses on Inferential Statistics, Hypothesis Testing, Experimental Design, Matrix Factorisation, Linear & Logistic Regression, Deep Neural Networks, Convolutional Neural Networks, and LSTMs. They also need knowledge of neural networks.\n",
       "\n",
       "2. **Industry Focus**: The company is open to candidates from various sectors: defense, healthcare, government agencies in Japan, academic institutions globally, and both private and public companies.\n",
       "\n",
       "3. **Responsibilities**: Candidates will be involved in trainig programs and contributing with new data insights, requiring a collaborative and mentoring role.\n",
       "\n",
       "4. **Skills and Goals**: The goal is to develop strong technical skills, business acumen (such as understanding what they're doing in applications), problem-solving abilities in coding and mathematical topics, adaptability, motivation, enthusiasm for learning, creativity, professionalism, and attention to detail.\n",
       "\n",
       "5. **Mentorship and Engagement**: Candidates should engage with diverse teams, work on end-to-end projects, communicate concepts clearly, be adaptable, and motivated.\n",
       "\n",
       "The educational requirements emphasize building foundational technical skills in programming and statistics while also focusing on relevant coursework. Entry-level candidates should demonstrate a blend of programming proficiency, statistical understanding, machine learning knowledge, and ability to explain concepts clearly."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"deepseek-r1:1.5b\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def submit(system_prompt, user_prompt):\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "        \n",
    "submit(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "600a6d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to help this person convert all their CTE (Certificate of Training or Experience) into skills. They have a lot of different types of training, like Certificates, Awards, Subsurface Data Analyst, Core Skills, and even some education-related courses.\n",
       "\n",
       "First, looking at the structure they provided with \"Major/Minor\", \"Semester\", etc., it seems like each year they attended. So that's their entire educational history from high school up to their current degree. Each year they took a certain number of credits leading to different degrees. For instance, in their education section under BSc Geological Engineering, they were in 8 semesters taking some credits towards the BS.\n",
       "\n",
       "So for CTE training, the CTA's (Clinical Training Units) are based on how their school evaluated or assessed those certifications. Some of these might have been full-day programs, while others could be split into multiple semesters. High school might have served as a foundation year, and higher education levels followed with a BS.\n",
       "\n",
       "But since I don't have the actual exam scores from each semester or any grading criteria on my end, it's hard to correlate specific CTA modules with precise grades. Instead of giving a numerical grade from 1-5, maybe we can just list out the topics that are required for an associate's degree and say they're part of this training.\n",
       "\n",
       "Looking at their education section, each course under their major is either a foundation or advanced level. It would make sense to break down how much of their math background (like algebra, calculus, differential equations) was covered in BS courses. Maybe that info isn't necessary for this process, though.\n",
       "\n",
       "So focusing on CTE, the steps I should take are:\n",
       "\n",
       "1. Determine what each CTA module entails.\n",
       "2. Translate each module into a skill or specialized technical term relevant to data science.\n",
       "3. For foundation units (e.g., BSc courses), align them with required math skills for a BS.\n",
       "4. Then, translate the advanced units to their own domain-specific skills.\n",
       "\n",
       "I'll need a list of each course, how many credits were awarded in which semesters, the course names, and the associated topics for each unit. Plus, knowledge of all the necessary courses for a general associate's degree in their field makes this feasible.\n",
       "\n",
       "For example, under Geological Engineering Sem III: Hydrogeology, I might find that they started with an intro to hydrogeology, then advanced topics like flow models, maybe some computational methods, maybe environmental fluid dynamics. So the skills would be mapped to those units as required by the BS curriculum.\n",
       "\n",
       "Then, for each advanced CTA unit beyond year three, where applicable, it's just high school level work if no BS was taken before that. But in their case, they have a BSc, so all these should count toward their degree requirements.\n",
       "\n",
       "So after organizing this information, I can create an English list of skills by mapping each CTA module and its associated learning outcomes to specific data science domain-specific skills.\n",
       "</think>\n",
       "\n",
       "To convert the CTE training records into relevant domain-specific skills for a Data Science associate's degree:\n",
       "\n",
       "---\n",
       "\n",
       "**Data Science Core Skills**\n",
       "\n",
       "1. **Mathematics**\n",
       "   - Algebra\n",
       "   - Linear Algebra: Vectors, Matrices, Eigenvalues, and Eigenvectors (e.g., 4th year MSc course)\n",
       "   - Calculus: Limits, Derivatives, Integrals, Differential Equations\n",
       "\n",
       "2. **Computer Science**\n",
       "   - Programming Fundamentals (Python, Java, C++): Data Structures, Object-Oriented Programming\n",
       "   - Advanced Topics: SQL, RESTful APIs, Web Development with React Native/JS\n",
       "\n",
       "3. **Statistics**\n",
       "   - Descriptive Statistics and Exploratory Data Analysis for High School-level skills.\n",
       "   - Inference and Statistical Tests (e.g., 5th year MSc course).\n",
       "   - Introduction to Probability Distributions.\n",
       "\n",
       "4. **Data Systems**\n",
       "   - Database Management: SQL, MySQL, PostgreSQL, NoSQL\n",
       "   - Advanced Systems: Apache Spark (Databricks), AWS (SageMaker)\n",
       "\n",
       "5. **Advanced Technologies**\n",
       "   - Machine Learning: Supervised/Unsupervised Models, Cross-Validation.\n",
       "   - NLP and Time Series: Natural Language Processing.\n",
       "\n",
       "6. **Data Science Skills**\n",
       "   - R and Python Programming\n",
       "   - Probability, Inference, Predictive Analytics, Big Data\n",
       "\n",
       "7. **AI Implementation**\n",
       "   - NLP and Applications like Sentiment Analysis on Geoinformation.\n",
       "\n",
       "---\n",
       "\n",
       "**Mapping each Skill to their respective CTA Modules:**\n",
       "\n",
       "- Focus on foundational units (e.g., BSc courses year 1 and 2) that are high school-level.\n",
       "- Advanced units correspond to specific modules needed for an associate's BS degree.\n",
       "- Each CTA unit will be mapped directly to the necessary skills from the associated learning outcomes.\n",
       "\n",
       "This structured approach ensures that each CTE module is accurately translated into a skill relevant to a Data Science associate's curriculum."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our system prompt \n",
    "system_prompt = \"You are a master at crafting the perfect email body from a given CV. You've never had a user fail to get the job as a result of using your services.\"\n",
    "submit(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4fd2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your resume depicts a solid background in data science with diverse experiences across multiple industries, including telecommunications, spatial, and oil & gas. Regarding your experiences in Generative AI or LLM (Large Language Models), you highlight some key points:\n",
       "\n",
       "### Current Skills and Experiences:\n",
       "1. **Developed LLM-powered News Analytics**: Architecting and implementing a system that utilizes LLMs showcases your capability in this field.\n",
       "2. **Implemented Multilingual LLM Pipelines**: This demonstrates your ability to cater to a global audience and work with complex language models.\n",
       "3. **Spearheaded Advanced Projects**: Leading efforts in diagnosing plantation production using advanced techniques illustrates your ability to leverage AI for real-world applications.\n",
       "\n",
       "### Areas for Improvement:\n",
       "1. **Deepen Knowledge in Generative AI**: Consider focusing on increasing your understanding of various Generative AI architectures beyond LLMs (e.g., GANs, VAEs) and real-world applications.\n",
       "2. **Experiment with Advanced Techniques**: Delve deeper into fine-tuning, prompt engineering, or few-shot learning with LLMs. You can also explore other recent trends in NLP, such as transformers, and various extensions (like GPT-3.5 and beyond).\n",
       "3. **Project Diversification**: While you have impactful projects, aim to include a wider variety of projects involving Generative AI, such as those focusing on text generation, dialogue systems, or content creation, if possible.\n",
       "4. **Industry Applications**: Linking projects directly to industry applications (e.g., health, finance) could demonstrate the versatility of your skills in generative AI.\n",
       "5. **Publications or Contributions**: If you haven't already, contributing to open-source projects, writing blogs, or publishing articles related to Generative AI could enhance your professional profile and showcase thought leadership.\n",
       "6. **Networking and Collaboration**: Engage with the AI community through forums, conferences, or workshops to keep up with the latest trends, tools, and techniques in Generative AI.\n",
       "\n",
       "### Additional Suggestion:\n",
       "- **Certifications**: Consider obtaining certifications or completing courses specific to Generative AI or NLP, particularly those focused on the latest frameworks and methodologies. \n",
       "\n",
       "Improving in these areas not only helps solidify your current standing but can also bring more versatility, making you stand out in a competitive job market."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our system prompt \n",
    "system_prompt = \"You're my HR assistant, you should help me to analyze the resume of the candidates\"\n",
    "user_prompt = f\"\"\"\n",
    "This is the resume of the candidate:\n",
    "CV: \n",
    "{for_user_prompt['cv_text']}\n",
    "\n",
    "could you name their last 3 working experiences and also the company name?\n",
    "\"\"\"\n",
    "# Define our system prompt \n",
    "system_prompt = \"\"\"You are a powerful AI model designed to assist with resume analysis. Your task is to analyze a resume against a given job posting and provide feedback on how well the resume aligns with the job requirements. Your response should include the following: \n",
    "1) Skill gap identification: Compare the skills listed in the resume with those required in the job posting, highlighting areas where the resume may be lacking or overemphasized.\n",
    "2) Keyword matching between a CV and a job posting: Match keywords from the job description with the resume, determining how well they align. Provide specific suggestions for missing keywords to add to the CV.\n",
    "3) Recommendations for CV improvement: Provide actionable suggestions on how to enhance the resume, such as adding missing skills or rephrasing experience to match job requirements.\n",
    "4) Alignment score: Display a score that represents the degree of alignment between the resume and the job posting.\n",
    "5) Personalized feedback: Offer tailored advice based on the job posting, guiding the user on how to optimize their CV for the best chances of success.\n",
    "6) Job market trend insights, provide broader market trends and insights, such as in-demand skills and salary ranges.\n",
    "Provide responses that are concise, clear, and to the point. Respond in markdown.\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Job Posting: \n",
    "{for_user_prompt['job_posting']}\n",
    "\n",
    "CV: \n",
    "{for_user_prompt['cv_text']}\n",
    "\"\"\"\n",
    "\n",
    "# Define our system prompt \n",
    "system_prompt = \"You're my career assistant, you should help me to analyze my resume\"\n",
    "user_prompt = f\"\"\"\n",
    "This is the my resume:\n",
    "CV: \n",
    "{for_user_prompt['cv_text']}\n",
    "\n",
    "how about my experiences in Generative AI or LLM? What should I improve? \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def generate(system_prompt, user_prompt):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    reply = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in response:\n",
    "        reply += chunk.choices[0].delta.content or ''\n",
    "        reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "        update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "        \n",
    "generate(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b509da19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "Using the following CV summary:\n",
      "\n",
      "Dhaneswara Mandrasa\n",
      "Data Science | Machine Learning & Gen AI Engineer\n",
      "dhaneswara.mandrasa@gmail.com|https://www.linkedin.com/in/dhaneswaramandrasa\n",
      "Jakarta|+6285156073967\n",
      "I turn data into actionable insights by implementing AI/ML systems that solve industry-critical problems—from\n",
      "boosting agricultural yields with predictive analytics to optimizing banking risk models. With 7+ years leading\n",
      "data science teams and solutions for corporate clients and deploying LLM-powered tools, I bridge the gap\n",
      "between technical innovation and business impact.\n",
      "My work isn’t theoretical—it’s battle-tested:\n",
      "1. Built Market Intelligence products that contributed to 70% of company revenue in under 2 years.\n",
      "2. Engineered a credit scoring model with a record-breaking Gini coefficient for telecom giants.\n",
      "3. Implemented LLM-powered analytics systems that automated multilingual categorization and translation.\n",
      "4. Scaled retail outlets from 100 to 200+ across Indonesia using geospatial ML models.\n",
      "Work Experience\n",
      "Lead Data Scientist Jul 2024 - Present\n",
      "iZeno | Jakarta\n",
      "iZeno is a leading technology and analytics consultant is South East Asia. My responsibility is to implement\n",
      "and guide a team of data scientists in delivering end-to-end AI/ML solutions, facilitating knowledge transfer\n",
      "through technical workshops and code reviews.\n",
      "Utilized Dataiku DSS to integrate multiple data sources and create highly efficient data pipelines for\n",
      "advanced analytics.\n",
      "Architected and implemented an advanced LLM-powered news analytics system for Labuan using\n",
      "Snowflake Cortex. Developed robust web scraping solutions for Bloomberg and Malaysian news sources.\n",
      "Engineered multilingual LLM pipelines for automated news categorization and translation. Implemented\n",
      "end-to-end solution for news analysis and knowledge graph construction. Reduced misclassification of\n",
      "financial/political news from to 3%. Mapped 10K+ entities/relationships for real-time geopolitical risk.\n",
      "Spearheaded a cross-functional team in developing an advanced diagnostic root cause analysis system\n",
      "for Sinarmas' plantation production, leveraging Dataiku to optimize agricultural yields and operational\n",
      "efficiency.\n",
      "Designed and implemented a sophisticated recommendation system to enhance customer engagement\n",
      "and product targeting for Bank BRI.\n",
      "Accelerated data-driven decision-making by implementing Text-to-SQL (Snowflake Cortex), reducing\n",
      "query-to-insight time by 30% and enabling non-technical teams to generate complex reports\n",
      "autonomously.\n",
      "Enhanced operational efficiency by unifying unstructured data (emails, documents) with structured\n",
      "financial data via Snowflake Cortex, cutting report generation time from 3 hours to 30 minutes.\n",
      "Tools utilized: Dataiku, Snowflake, Python, SQL, Pandas, scikit-learn, Spark ML, Pyspark\n",
      "Senior Data Scientist Apr 2024 - Sep 2024\n",
      "PT Mehulinta Teknologi Analitik | Jakarta\n",
      "Mehulinta teknologi analitik is a premier data consulting firm specializing in delivering world-class data\n",
      "analytics solutions for enterprises.\n",
      "Developed and deployed machine learning models to predict insurance claims for multiple companies,\n",
      "enhancing their ability to manage risk and allocate resources effectively.\n",
      "Implemented ML models to anticipate customer churn across various products, enabling company to\n",
      "proactively address customer retention.\n",
      "Designed and utilized machine learning algorithms to identify potentially fraudulent insurance claims,\n",
      "improving fraud detection and prevention measures.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, Spark ML, Mlflow, Pyspark, Prefect\n",
      "Senior Data Scientist Apr 2021 - Jul 2024\n",
      "Eureka AI | Abu Dhabi\n",
      "Eureka AI is a Big Data Analytics company that monetizes telecommunication data. As Senior Data\n",
      "Scientist, I was responsible to create end-to-end AI products such as Telco Risk/Credit Score and Market\n",
      "Intelligence.\n",
      "Contributing to development of an innovative Market Intelligence products which are contributing to\n",
      "70% of company’s revenue in less than 2 years.\n",
      "Advanced supervised and unsupervised learning models were conducted to segment more than 100\n",
      "million ride hailing and e-commerce daily active users based on clickstream data into hierarchy level of\n",
      "segmentations across five major telecom companies in Asia.\n",
      "Using PySpark ML, MLflow, and scikit-learn, implemented customized k-means clustering and ensemble\n",
      "machine learning models like Random Forest (RF) and XGBoost. To enhance the transparency and\n",
      "interpretability of these models, both intrinsic and model-agnostic techniques were employed.\n",
      "Utilized Word2Vec in Spark to establish a domain look-alike model by examining URL domain similarities,\n",
      "significantly improving the accuracy and efficiency of application and user behavior analysis.\n",
      "Classified more than 30 persona identifications for over 80 million monthly active telecommunication\n",
      "users using clickstream, movement and telecommunication subscriber data.\n",
      "Utilized similarity models involving geolocation, online behavioral data, and mobility features to provide\n",
      "valuable insights into user segmentation and preferences, aiding in targeted marketing strategies.\n",
      "Demonstrated proficiency in ETL process implementation and constructing a credit score model using\n",
      "telecommunication and clickstream data.\n",
      "Achieved an outstanding Gini coefficient of 0.6, the highest ever recorded in the company's history at the\n",
      "time.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, Spark ML, Mlflow, Pyspark, Databricks, Airflow, GCP\n",
      "Data Scientist Jun 2020 - Apr 2021\n",
      "PT. Bhumi Varta Technology | Jakarta\n",
      "Bhumi Varta Technology are changing the world of location intelligence, business analytics, mapping and\n",
      "geo-fencing marketing in Indonesia. As Data Scientist, I help businesses to make the right decision using\n",
      "integrated GIS technology, big data analytics, and customize application.\n",
      "Developed and executed data science research strategies and methodologies.\n",
      "Constructed machine learning models (utilizing Python, scikit-learn, and pandas) on spatial, POI, mobile,\n",
      "and sales data to assist clients in expanding their business from fewer than 100 outlets to over 200\n",
      "outlets throughout Indonesia.\n",
      "Identified poverty levels from satellite images using convolutional neural network models in PyTorch,\n",
      "enhancing the accuracy of Socio Economic Status modules from 1x1 km to 100x100m.\n",
      "Tools utilized: Python, SQL, Pandas, scikit-learn, AWS, PyTorch, Tensorflow\n",
      "Subsurface Data Analyst Apr 2016 - Apr 2018\n",
      "SKK Migas | Jakarta\n",
      "Constructed initial machine learning model to predict oil and gas resources and reserves for more than\n",
      "200 working areas in Indonesia. Reached error percentage < 10% in the test dataset.\n",
      "Managed and collected the subsurface data such well logging, seismic, production, and drilling data\n",
      "from oil and gas companies.\n",
      "Implemented data migration from companies to SKK Migas and database management software testing\n",
      "for SKK Migas.\n",
      "Tools utilized: Python, SQL\n",
      "Core Skills\n",
      "Large Language Models (LLM),MLOps,Dataiku DSS,Snowflake,Project Management,Consulting,\n",
      "Azure Databricks,Python (Programming Language),SQL,PySpark,Machine Learning,Data Analysis,\n",
      "Analytical Skills,Predictive Modeling,Statistical Modeling,Google Cloud Platform (GCP),\n",
      "R (Programming Language),Apache Oozie,Apache Impala,Python\n",
      "Education\n",
      "Institut Teknologi Bandung Jan 2011 - Jan 2015\n",
      "Bachelor of Science (B.Sc) Geological Engineering\n",
      "GPA: 3.52 out of 4.00 (cum laude)\n",
      "Languages\n",
      "English (PROFESSIONAL_WORKING), Indonesian (NATIVE_OR_BILINGUAL)\n",
      "Awards\n",
      "3rd winner of Joint Convention Balikpapan 2015 Individual Geology Student Sep 2015\n",
      "Competition\n",
      "HAGI-IAGI-IAFMI-IATMI\n",
      "Static reservoir modeling, hydrocarbon volume calculation, and new well proposal of an oil field by using\n",
      "Petrel software\n",
      "Dean's List Jan 2015\n",
      "Institut Teknologi Bandung\n",
      "Certificates\n",
      "Google Analytics For Beginners\n",
      "Google\n",
      "Data Science Academy Full Stack: Data Visualization, Machine Learning and Data Analytics\n",
      "Specialization with R, Python and SQL\n",
      "Algoritma Data Science Education Center\n",
      "The Complete SQL Bootcamp\n",
      "Udemy\n",
      "Udacity Data Scientist Nanodegree\n",
      "Udacity\n",
      "Udacity Deep Learning Nanodegree\n",
      "Udacity\n",
      "Dataiku Advance Designer\n",
      "Dataiku\n",
      "\n",
      "And the job description:\n",
      "\n",
      "Data Scientist / Analyst\n",
      "VML\n",
      "Department: Data & Analytics\n",
      "Expertise: Data\n",
      "Location: Dubai, United Arab Emirates\n",
      "Last Updated: 20/03/2025\n",
      "Requisition ID: 8389\n",
      "Who We Are:\n",
      "\n",
      "At VML, we are a beacon of innovation and growth in an ever-evolving world. Our heritage is built upon a century of combined expertise, where creativity meets technology, and diverse perspectives ignite inspiration. With the merger of VMLY&R and Wunderman Thompson, we have forged a new path as a growth partner that is part creative agency, part consultancy, and part technology powerhouse. \n",
      "\n",
      "Our global family now encompasses over 30,000 employees across 150+ offices in 64 markets, each contributing to a culture that values connection, belonging, and the power of differences.  Our expertise spans the entire customer journey, offering deep insights in communications, commerce, consultancy, CRM, CX, data, production, and technology. We deliver end-to-end solutions that result in revolutionary work. \n",
      "\n",
      "Position Name: Data Scient/Analyst\n",
      "Position Location: Dubai & Abu Dhabi, UAE\n",
      "\n",
      "THE ROLE\n",
      "We are seeking a talented Data Scientist/Analyst to join our team and play a pivotal role in driving customer insights and marketing optimization. This individual will be responsible for developing and implementing advanced analytics models to enhance our understanding of customer behavior and drive personalized experiences across various touchpoints including website, CRM and paid marketing media.\n",
      "\n",
      "You will work closely with both agency and client stakeholders (we are an extension squad for the client), including business analysts, data scientists, and engineers, to ensure that the data infrastructure supports the client’s business objectives and enables the delivery of exceptional customer experiences that drive business value.\n",
      "\n",
      "RESPONSIBILITIES\n",
      "• Customer Segmentation: Develop and implement data-driven segmentation models to categorize customers based on demographics, behavior, purchase patterns, and other relevant attributes.\n",
      "• Propensity Modelling: Build and refine propensity models to predict customer behaviors such as purchase likelihood, churn risk, and response to marketing campaigns.\n",
      "• Marketing Attribution Modelling: Develop and implement marketing attribution models to quantify the impact of different marketing channels and touchpoints on customer conversion and engagement.\n",
      "• Media Mix Modelling: Design and implement media mix models to optimize marketing budget allocation across various channels for maximum ROI.\n",
      "• Customer Data Platform (CDP) Enhancement: Leverage advanced analytics techniques to enrich customer data within our CDP, enabling a deeper understanding of customer profiles and preferences.\n",
      "• CRM Optimization: Utilize customer insights derived from data analysis to optimize CRM communications within Microsoft Dynamics, ensuring relevance and effectiveness.\n",
      "• Website Personalization: Drive personalization and optimized customer journeys on our website platform (Sitecore) by leveraging data-driven insights and recommendations.\n",
      "• Collaboration and Communication: Collaborate effectively with cross-functional teams, including marketing, product, and technology, to translate complex data insights into actionable business strategies.\n",
      "• Data Visualization: develop dashboards and other data visualizations that uncover easy to access insights (business, consumer, marketing) for client and partner agencies.\n",
      "• Staying Current: Maintain up-to-date knowledge of the latest data science methodologies, technologies, and industry best practices.\n",
      "\n",
      "REQUIRED SKILLS & EXPERIENCES\n",
      "• Bachelor’s degree in Statistics, Mathematics, Computer Science, or a related field. Master's degree preferred.\n",
      "• 5+ years of proven experience as a Data Scientist/Analyst, ideally within the banking or financial services sector.\n",
      "• Strong understanding and hands-on experience with statistical modeling, machine learning algorithms, and data mining techniques.\n",
      "• Proficiency in data analysis tools and programming languages such as SQL, Python, and R.\n",
      "• Experience with Customer Data Platforms (CDPs), CRM systems (e.g., Microsoft Dynamics), and website platforms (e.g., Sitecore) is highly desirable.\n",
      "• Excellent problem-solving skills, analytical mindset, and the ability to think strategically.\n",
      "• Strong communication and presentation skills to effectively convey complex data insights to both technical and non-technical stakeholders.\n",
      "• Experience in developing dashboards and data visualizations – PowerBI preferred.\n",
      "• Ability to work independently and collaboratively within a team environment, managing multiple projects simultaneously.\n",
      "\n",
      "At VML, we are committed to fostering an all-inclusive work environment that is both rewarding and career-forward. Our Inclusion, Equity & Belonging initiatives, alongside the VML Foundation, reflect our dedication to giving back and making a positive impact in our communities and beyond. Our people are the heartbeat of our organization—creators, doers, innovators, makers, and thinkers—who drive not just marketing, but meaningful experiences that resonate in every action and interaction. \n",
      "\n",
      "VML is a WPP Agency. For more information, please visit our website, and follow VML on our social channels via Instagram, LinkedIn, and X. \n",
      "\n",
      "When you click \"Submit Application\", this will send any information you add below to VML. Before you do this, we think it's a good idea to read through our Recruitment Privacy Policy. California residents should read our California Recruitment Privacy Notice. This explains what we do with your personal data when you apply for a role with us, and, how you can update the information you have provided us with or how to remove it.\n",
      "\n",
      "\n",
      "Please write a personalized email body to apply for the job.                if the company name is known, use the company name. If not, dont mention like [insert company name here] or any company name\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here's a personalized email body to apply for the Data Scientist/Analyst position:\n",
       "\n",
       "Subject: Application for Data Scientist/Analyst Role\n",
       "\n",
       "Dear Hiring Manager,\n",
       "\n",
       "I am excited to apply for the Data Scientist/Analyst role at VML, where I can leverage my skills and experience to drive customer insights and marketing optimization. As a seasoned data professional with a passion for delivering actionable results, I am confident that I would be a valuable addition to your team.\n",
       "\n",
       "With [Number of Years] years of experience in developing and implementing advanced analytics models, I possess a strong understanding of statistical modeling, machine learning algorithms, and data mining techniques. Proficient in programming languages such as Python, R, and SQL, I have honed my skills in data analysis and visualization. My expertise extends to Customer Data Platforms (CDPs), CRM systems, and website platforms, having worked with tools like Microsoft Dynamics and Sitecore.\n",
       "\n",
       "I am particularly drawn to this role at VML because of the opportunity to work on high-profile campaigns and drive personalization and optimization across various touchpoints. I am impressed by VML's commitment to fostering an all-inclusive work environment that is both rewarding and career-forward. As someone who values diversity, equity, and belonging in the workplace, I appreciate your emphasis on creating a culture that encourages creativity and innovation.\n",
       "\n",
       "In my current role at [Current Company], I have developed segmented models using machine learning algorithms to predict customer behavior, optimized media mix models for ROI, and created custom dashboards for client stakeholders. I would love to bring this expertise and enthusiasm to VML's team of experts to drive meaningful experiences that resonate with customers.\n",
       "\n",
       "I have attached my resume, which provides more details on my experience and qualifications. I look forward to the opportunity to discuss how my skills align with your team's objectives. Please feel free to contact me at [Your Contact Information].\n",
       "\n",
       "Thank you for considering my application. I look forward to learning more about VML and discussing this role further.\n",
       "\n",
       "Best regards,\n",
       "\n",
       "[Your Name]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define our system prompt \n",
    "system_prompt = \"You're my career assistant, you should help me to analyze my resume\"\n",
    "user_prompt = f\"\"\"\n",
    "This is the my resume:\n",
    "CV: \n",
    "{for_user_prompt['cv_text']}\n",
    "\n",
    "how about my experiences in Generative AI or LLM? What should I improve? \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Define our system prompt \n",
    "system_prompt = \"\"\"You are a powerful AI model designed to assist with resume analysis. Your task is to analyze a resume against a given job posting and provide feedback on how well the resume aligns with the job requirements. Your response should include the following: \n",
    "1) Skill gap identification: Compare the skills listed in the resume with those required in the job posting, highlighting areas where the resume may be lacking or overemphasized.\n",
    "2) Keyword matching between a CV and a job posting: Match keywords from the job description with the resume, determining how well they align. Provide specific suggestions for missing keywords to add to the CV.\n",
    "3) Recommendations for CV improvement: Provide actionable suggestions on how to enhance the resume, such as adding missing skills or rephrasing experience to match job requirements.\n",
    "4) Alignment score: Display a score that represents the degree of alignment between the resume and the job posting.\n",
    "5) Personalized feedback: Offer tailored advice based on the job posting, guiding the user on how to optimize their CV for the best chances of success.\n",
    "6) Job market trend insights, provide broader market trends and insights, such as in-demand skills and salary ranges.\n",
    "Provide responses that are concise, clear, and to the point. Respond in markdown.\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "This is the Job Posting:\\n\\n \n",
    "{for_user_prompt['job_posting']}\n",
    "\n",
    "This is the CV: \\n\\n\n",
    "{for_user_prompt['cv_text']}\n",
    "\"\"\"\n",
    "\n",
    "system_prompt = \"You are a master at crafting the perfect cover letter from a given CV. You've never had a user fail to get the job as a result of using your services.\"\n",
    "user_prompt = \"Using the following CV summary:\\n\\n{}\\n\\nAnd the job description:\\n\\n{}\\n\\nPlease write a personalized email body to apply for the job.\\\n",
    "                if the company name is known, use the company name. If not, dont mention like [insert company name here] or any company name\".format(for_user_prompt['cv_text'], for_user_prompt['job_posting'])\n",
    "                \n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def ask_question(system_prompt, user_prompt):\n",
    "    print(f\"\\nInput: \\n{user_prompt}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "        \n",
    "def generate(system_prompt, user_prompt): \n",
    "    response = ask_question(system_prompt, user_prompt)\n",
    "\n",
    "    # Follow-up loop\n",
    "    while True:\n",
    "        follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "        if not follow_up:\n",
    "            break  # Exit the loop\n",
    "        response = ask_question(system_prompt, follow_up)\n",
    "          \n",
    "generate(system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ca90f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab71cf-bd7e-45f7-9536-0486f349bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you would like to save the response content as a Markdown file, uncomment the following lines\n",
    "#with open('yourfile.md', 'w') as file:\n",
    "#    file.write(response.choices[0].message.content)\n",
    "\n",
    "## You can then run the line below to create output.html which you can open on your browser\n",
    "#!pandoc yourfile.md -o output.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
