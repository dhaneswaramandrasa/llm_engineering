{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv()\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! The code you provided is a Python expression that uses a combination of a generator function and a set comprehension. Let's break it down step by step:\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   - This part of the code creates a set using a set comprehension.\n",
       "   - `books` is assumed to be an iterable (like a list) containing dictionaries, where each dictionary represents a book.\n",
       "   - `book.get(\"author\")` retrieves the value associated with the key `\"author\"` from each book dictionary.\n",
       "   - The comprehension iterates over each `book` in `books`.\n",
       "   - The `if book.get(\"author\")` condition filters out any books that do not have an author (i.e., if the author's name is `None` or an empty string, those books are excluded).\n",
       "   - The end result is a set of unique author names, because sets automatically handle duplicates.\n",
       "\n",
       "2. **Yielding from the Set**:\n",
       "   python\n",
       "   yield from ...\n",
       "   \n",
       "   - The `yield from` statement is used within a generator function to yield values from another iterable (in this case, the set created by the set comprehension).\n",
       "   - When `yield from` is used, the generator will yield each value from the provided iterable one by one. This allows for a clean way to delegate part of the generator's operation to another iterable.\n",
       "  \n",
       "### Complete Function Context\n",
       "\n",
       "For this `yield from` expression to work, it must be part of a generator function. An example of such a function might look like this:\n",
       "\n",
       "python\n",
       "def get_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "### What the Code Does\n",
       "\n",
       "- **Produces Unique Authors**: The entire line effectively creates a generator that produces unique author names from the list of book dictionaries.\n",
       "- **Handles Missing Data**: If a book does not have an `\"author\"` key or if the value is `None` or an empty string, that book is excluded from the final output.\n",
       "- **Efficient Memory Usage**: Since the result is yielded one at a time, it can be used in a memory-efficient way; you don't need to store the entire list of authors in memory if you process them one by one.\n",
       "\n",
       "### Why Use This Code?\n",
       "\n",
       "1. **Readability**: Using a generator with `yield from` makes the code succinct and easy to read.\n",
       "2. **Efficiency**: Generators are more memory efficient than lists since they generate values on-the-fly, which can be particularly useful if the `books` list is very large.\n",
       "3. **Elimination of Duplicates**: Creating a set inherently takes care of duplicate author names, ensuring that every author is returned only once.\n",
       "\n",
       "### Example Usage\n",
       "\n",
       "Here’s a quick example of how this function might be used:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": None},\n",
       "    {\"title\": \"Book Three\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Four\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "\n",
       "\n",
       "**Output**:\n",
       "\n",
       "Author A\n",
       "Author B\n",
       "\n",
       "\n",
       "In this example, both instances of \"Author A\" will only show up once in the output, demonstrating the functionality of the generator combined with set comprehensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[{\n",
    "        \"role\":\"system\",\n",
    "        \"content\":system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":user_prompt\n",
    "    }\n",
    "    ]\n",
    "    , stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b20e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "what is huff model in less than 50 words\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The HUFF model is a spatial market analysis tool used in retail and urban planning. It estimates consumer behavior by calculating the probability of a shopper choosing a particular store based on its distance and attractiveness, combining factors like size and type of retail offering."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "could we do this in python?\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Of course! I'd be happy to help you with that. Please provide more details about what you would like to accomplish in Python, and I'll do my best to assist you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "I mean is there any library for this?\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Could you please provide more context or specify what you're looking for? There are many libraries available in Python for various tasks, including data analysis, machine learning, web development, and more. Let me know what specific functionality you need, and I can recommend a suitable library!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "yes like scikit learn library\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Scikit-learn is a popular Python library used for machine learning and data science. It provides a wide range of tools for tasks such as classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. Here are some key features and common functionalities of scikit-learn:\n",
       "\n",
       "1. **Supervised Learning Algorithms**: Scikit-learn includes various algorithms for supervised learning tasks, such as linear regression, logistic regression, support vector machines, decision trees, random forests, and more.\n",
       "\n",
       "2. **Unsupervised Learning**: You can also perform unsupervised learning tasks, including clustering (e.g., K-means, hierarchical clustering) and dimensionality reduction (e.g., PCA, t-SNE).\n",
       "\n",
       "3. **Model Selection**: Scikit-learn provides tools for model evaluation and selection, including cross-validation techniques and metrics to assess model performance (accuracy, precision, recall, etc.).\n",
       "\n",
       "4. **Preprocessing**: The library includes functions for data preprocessing, such as scaling features, encoding categorical variables, and handling missing values.\n",
       "\n",
       "5. **Pipelines**: You can create pipelines to streamline the process of applying transformations and fitting models, which helps ensure reproducibility and maintainability.\n",
       "\n",
       "6. **Integration with Other Libraries**: Scikit-learn works well with other scientific computing libraries in Python, such as NumPy, pandas, and Matplotlib.\n",
       "\n",
       "Here’s a simple example of using scikit-learn to perform a classification task:\n",
       "\n",
       "python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.metrics import accuracy_score\n",
       "\n",
       "# Load a dataset (using Iris dataset as an example)\n",
       "from sklearn.datasets import load_iris\n",
       "data = load_iris()\n",
       "X = data.data\n",
       "y = data.target\n",
       "\n",
       "# Split the data into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Train a Random Forest Classifier\n",
       "model = RandomForestClassifier()\n",
       "model.fit(X_train, y_train)\n",
       "\n",
       "# Make predictions\n",
       "y_pred = model.predict(X_test)\n",
       "\n",
       "# Evaluate the model\n",
       "accuracy = accuracy_score(y_test, y_pred)\n",
       "print(f'Accuracy: {accuracy:.2f}')\n",
       "\n",
       "\n",
       "If you have specific questions about scikit-learn or need help with particular tasks or concepts, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "then how to do huff model using python\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Hierarchical Unified Feature Frequency (HUFF) model, commonly referred to as the HUFF model, is often associated with applications in natural language processing and data encoding due to its associative mapping of features and hierarchical organization. However, the terminology \"HUFF model\" could be somewhat ambiguous since it might refer to various concepts, including Huffman coding or Hierarchical models in general. \n",
       "\n",
       "If you're referring to a specific method (like Huffman coding, which is used for data compression), I can provide a Python implementation for that. Here's a simple example of how to implement Huffman coding in Python:\n",
       "\n",
       "### Huffman Coding Implementation in Python\n",
       "\n",
       "python\n",
       "import heapq\n",
       "from collections import defaultdict\n",
       "\n",
       "class Node:\n",
       "    def __init__(self, char, freq):\n",
       "        self.char = char\n",
       "        self.freq = freq\n",
       "        self.left = None\n",
       "        self.right = None\n",
       "\n",
       "    def __lt__(self, other):\n",
       "        return self.freq < other.freq\n",
       "\n",
       "def build_huffman_tree(text):\n",
       "    frequency = defaultdict(int)\n",
       "    \n",
       "    # Count frequency of each character\n",
       "    for char in text:\n",
       "        frequency[char] += 1\n",
       "\n",
       "    # Create a priority queue\n",
       "    priority_queue = [Node(char, freq) for char, freq in frequency.items()]\n",
       "    heapq.heapify(priority_queue)\n",
       "\n",
       "    # Merge nodes until there is only one node left\n",
       "    while len(priority_queue) > 1:\n",
       "        left = heapq.heappop(priority_queue)\n",
       "        right = heapq.heappop(priority_queue)\n",
       "        merged = Node(None, left.freq + right.freq)\n",
       "        merged.left = left\n",
       "        merged.right = right\n",
       "        heapq.heappush(priority_queue, merged)\n",
       "\n",
       "    return priority_queue[0]\n",
       "\n",
       "def build_codes(node, prefix=\"\", codebook={}):\n",
       "    if node:\n",
       "        if node.char is not None:\n",
       "            codebook[node.char] = prefix\n",
       "        build_codes(node.left, prefix + \"0\", codebook)\n",
       "        build_codes(node.right, prefix + \"1\", codebook)\n",
       "    return codebook\n",
       "\n",
       "def huffman_encoding(text):\n",
       "    root = build_huffman_tree(text)\n",
       "    huffman_codes = build_codes(root)\n",
       "    \n",
       "    encoded_output = ''.join(huffman_codes[char] for char in text)\n",
       "    \n",
       "    return encoded_output, huffman_codes\n",
       "\n",
       "def huffman_decoding(encoded_text, huffman_codes):\n",
       "    reverse_codes = {v: k for k, v in huffman_codes.items()}\n",
       "    current_code = \"\"\n",
       "    decoded_output = \"\"\n",
       "\n",
       "    for bit in encoded_text:\n",
       "        current_code += bit\n",
       "        if current_code in reverse_codes:\n",
       "            decoded_output += reverse_codes[current_code]\n",
       "            current_code = \"\"\n",
       "    \n",
       "    return decoded_output\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    text = \"this is an example for huffman encoding\"\n",
       "    print(\"Original Text: \", text)\n",
       "\n",
       "    encoded_text, huffman_codes = huffman_encoding(text)\n",
       "    print(\"Encoded Text: \", encoded_text)\n",
       "    print(\"Huffman Codes: \", huffman_codes)\n",
       "\n",
       "    decoded_text = huffman_decoding(encoded_text, huffman_codes)\n",
       "    print(\"Decoded Text: \", decoded_text)\n",
       "\n",
       "\n",
       "### Explanation:\n",
       "1. **Node Class**: Defines a node in the Huffman tree.\n",
       "2. **build_huffman_tree**: Creates a tree based on frequency of each character.\n",
       "3. **build_codes**: Generates binary codes for each character based on the tree structure.\n",
       "4. **huffman_encoding**: Encodes the input text into a binary representation.\n",
       "5. **huffman_decoding**: Decodes the binary representation back into the original text.\n",
       "\n",
       "### Usage:\n",
       "You can run the script, and it will encode the sample text and then decode it back to the original text.\n",
       "\n",
       "If this doesn't align with what you're asking for regarding \"HUFF model,\" please provide more context or details about the specific model or application you're referring to!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "what's the difference between Generative AI and LLM\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Generative AI (Artificial Intelligence) and LLM (Large Language Models) are related but not exactly the same thing.\n",
       "\n",
       "**Generative AI**: Generative AI refers to a class of machine learning algorithms that generate new, original content in various forms, such as:\n",
       "\n",
       "* Images: generating new images based on patterns and styles learned from existing images\n",
       "* Text: generating new text based on patterns and structures learned from existing text data\n",
       "* Music: generating new music based on patterns and melodies learned from existing music data\n",
       "\n",
       "Generative AI models are often trained using unsupervised learning methods, such as Generative Adversarial Networks (GANs) or Variational Autoencoders (VAEs), which allows them to learn complex patterns in the data without explicit human supervision.\n",
       "\n",
       "**LLM (Large Language Models)**: LLMs, on the other hand, are a specific type of deep learning model that is particularly well-suited for natural language processing tasks. An LLM is trained using a massive amount of text data and aims to predict the probability of a given sequence of words.\n",
       "\n",
       "While all LLMs can generate new text, not all generative AI models are necessarily LLMs. In other words, not all AI systems that generate text or images are Large Language Models.\n",
       "\n",
       "**Key differences**:\n",
       "\n",
       "* **Training objective**: Generative AI models aim to generate new content, whereas LLMs primarily focus on predicting probabilities in sequential data.\n",
       "* **Task-oriented design**: Generative AI models might be designed to perform specific tasks like image-to-image translation, while LLMs are typically used for text-to-text or question-answering applications.\n",
       "* **Data requirements**: Both require large amounts of data for training, but the type and structure of the data differ. LLMs rely on massive amounts of text data, whereas generative AI models might use a mix of labeled and unlabeled data.\n",
       "\n",
       "Example scenarios where you'd use each:\n",
       "\n",
       "* Use Generative AI (e.g., GANs) to:\n",
       "\t+ Generate new images for product design or advertising.\n",
       "\t+ Create music tracks for a specific style or genre.\n",
       "* Use LLMs (e.g., BERT, RoBERTa):\n",
       "\t+ Answer complex questions with relevant text from a knowledge base.\n",
       "\t+ Perform text classification or sentiment analysis.\n",
       "\n",
       "In summary: while both Generative AI and LLM are powerful tools for generating novel content, they have distinct design objectives and training requirements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65a1a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "hi, what's the difference between LLM and Generative AI. Elaborate please.\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, I'm trying to understand the difference between LLM (Large Language Models) and Generative AI. From what I know, both seem related to generating text or language-based outputs. But wait, isn't an LLM more technical and maybe narrower in scope? Maybe it's similar but has specific components or functionalities that a generative AI might incorporate.\n",
       "\n",
       "In my experience, I've seen tools like ChatGPT use something called the \"LLC\" module. That seems really important because it makes models more transparent. Without transparency, it's hard to trust how they generate responses, even if someone didn't realize they were being monitored or influenced by some unseen factors. So maybe the LLC adds a layer of control and responsibility to the model outputs.\n",
       "\n",
       "Generative AI, on the other hand, might handle tasks that aren't only natural language processing but could also include things like creative content generation. For example, an AI-powered editor could create images or music based on user inputs. While this is similar in concept to an LLM, it's more about applying advanced technologies specifically for creative purposes.\n",
       "\n",
       "Wait, so maybe every Generative AI model has the core functionalities of an LLM but can extend beyond that into other areas like art, design, etc.? That could make them even more versatile or \"generative\" because their outputs aren't just words or text but also images or other creative outputs. So in a way, they might be similar, but Generative AI seems to have broader applications and specific focus areas not commonly covered under traditional LLMs.\n",
       "</think>\n",
       "\n",
       "LLM (Large Language Model) and Generative AI are closely related concepts in the field of artificial intelligence, particularly in natural language processing (NLP). While they share a common foundation, there are notable differences in their scope, functionality, and intended use cases.\n",
       "\n",
       "**Generative AI:**\n",
       "- **Core Functionality:** Generative AI models focused on NLP tasks primarily generate text or descriptions from input data. These models can process unstructured data like HTML or JSON (using OpenAI's Text Generation API). They are versatile tools for summarization, translation, and creative content generation.\n",
       "- **Applications:** Beyond pure text generation, GANs (Generative Adversarial Networks) in AI art and generative music systems are examples where the model goes beyond text. These models can explore creative domains and offer specific applications that a more general language model might not cover.\n",
       "\n",
       "**Large Language Model (LLM):**\n",
       "- **Core Functionality:** An LLM is typically designed to process natural language tasks with high efficiency, focusing on generating responses or understanding context.\n",
       "- **Importance of Transparency:** An essential component of an LLM is the \"LLC\" module in platforms like ChatGPT. This module enhances transparency by allowing users to trace back how outputs are generated, which is crucial for building trust and preventing misuse.\n",
       "\n",
       "**Key Features Distinct:\n",
       "\n",
       "1. **Generative Nature:**\n",
       "   - Generative AI goes beyond text generation. GAN-based systems can create images or music based on prompts (e.g., \"Create a cute dog scene\").\n",
       "   \n",
       "2. **Focus Beyond Text:**\n",
       "   - An LLM is more strictly tied to NLP tasks, focusing on language processing.\n",
       "   - A Generative AI system might involve specialized components for creative outputs, expanding its functionality.\n",
       "\n",
       "**Conclusion:**\n",
       "Generative AI extends the functionality of models beyond plain text generation, offering versatility across different domains. While an LLM is a subset, focusing on NLP with added transparency features, GANs in AI are examples where this concept transcends text and into creativity.\n",
       "\n",
       "Thus, Generative AI can be seen as an application of advanced techniques within General AI frameworks, while the LLM represents a specific instance with different focus areas."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"deepseek-r1:1.5b\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba59283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
