{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "load_dotenv()\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "system_prompt = \"You are a helpful technical tutor who answers questions about python code, software engineering, data science and LLMs\"\n",
    "user_prompt = \"Please give a detailed explanation to the following question: \" + question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! The code you provided is a Python expression that uses a combination of a generator function and a set comprehension. Let's break it down step by step:\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   \n",
       "   - This part of the code creates a set using a set comprehension.\n",
       "   - `books` is assumed to be an iterable (like a list) containing dictionaries, where each dictionary represents a book.\n",
       "   - `book.get(\"author\")` retrieves the value associated with the key `\"author\"` from each book dictionary.\n",
       "   - The comprehension iterates over each `book` in `books`.\n",
       "   - The `if book.get(\"author\")` condition filters out any books that do not have an author (i.e., if the author's name is `None` or an empty string, those books are excluded).\n",
       "   - The end result is a set of unique author names, because sets automatically handle duplicates.\n",
       "\n",
       "2. **Yielding from the Set**:\n",
       "   python\n",
       "   yield from ...\n",
       "   \n",
       "   - The `yield from` statement is used within a generator function to yield values from another iterable (in this case, the set created by the set comprehension).\n",
       "   - When `yield from` is used, the generator will yield each value from the provided iterable one by one. This allows for a clean way to delegate part of the generator's operation to another iterable.\n",
       "  \n",
       "### Complete Function Context\n",
       "\n",
       "For this `yield from` expression to work, it must be part of a generator function. An example of such a function might look like this:\n",
       "\n",
       "python\n",
       "def get_authors(books):\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "\n",
       "### What the Code Does\n",
       "\n",
       "- **Produces Unique Authors**: The entire line effectively creates a generator that produces unique author names from the list of book dictionaries.\n",
       "- **Handles Missing Data**: If a book does not have an `\"author\"` key or if the value is `None` or an empty string, that book is excluded from the final output.\n",
       "- **Efficient Memory Usage**: Since the result is yielded one at a time, it can be used in a memory-efficient way; you don't need to store the entire list of authors in memory if you process them one by one.\n",
       "\n",
       "### Why Use This Code?\n",
       "\n",
       "1. **Readability**: Using a generator with `yield from` makes the code succinct and easy to read.\n",
       "2. **Efficiency**: Generators are more memory efficient than lists since they generate values on-the-fly, which can be particularly useful if the `books` list is very large.\n",
       "3. **Elimination of Duplicates**: Creating a set inherently takes care of duplicate author names, ensuring that every author is returned only once.\n",
       "\n",
       "### Example Usage\n",
       "\n",
       "Here’s a quick example of how this function might be used:\n",
       "\n",
       "python\n",
       "books = [\n",
       "    {\"title\": \"Book One\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book Two\", \"author\": None},\n",
       "    {\"title\": \"Book Three\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book Four\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "\n",
       "\n",
       "**Output**:\n",
       "\n",
       "Author A\n",
       "Author B\n",
       "\n",
       "\n",
       "In this example, both instances of \"Author A\" will only show up once in the output, demonstrating the functionality of the generator combined with set comprehensions."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model=MODEL_GPT,\n",
    "    messages=[{\n",
    "        \"role\":\"system\",\n",
    "        \"content\":system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":user_prompt\n",
    "    }\n",
    "    ]\n",
    "    , stream=True\n",
    ")\n",
    "\n",
    "response = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    response += chunk.choices[0].delta.content or \"\"\n",
    "    response = response.replace(\"```\",\"\").replace(\"markdown\", \"\")\n",
    "    update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b20e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "what is huff model in less than 50 words\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The HUFF model is a spatial market analysis tool used in retail and urban planning. It estimates consumer behavior by calculating the probability of a shopper choosing a particular store based on its distance and attractiveness, combining factors like size and type of retail offering."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "could we do this in python?\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Of course! I'd be happy to help you with that. Please provide more details about what you would like to accomplish in Python, and I'll do my best to assist you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "I mean is there any library for this?\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Could you please provide more context or specify what you're looking for? There are many libraries available in Python for various tasks, including data analysis, machine learning, web development, and more. Let me know what specific functionality you need, and I can recommend a suitable library!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "yes like scikit learn library\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Scikit-learn is a popular Python library used for machine learning and data science. It provides a wide range of tools for tasks such as classification, regression, clustering, dimensionality reduction, model selection, and preprocessing. Here are some key features and common functionalities of scikit-learn:\n",
       "\n",
       "1. **Supervised Learning Algorithms**: Scikit-learn includes various algorithms for supervised learning tasks, such as linear regression, logistic regression, support vector machines, decision trees, random forests, and more.\n",
       "\n",
       "2. **Unsupervised Learning**: You can also perform unsupervised learning tasks, including clustering (e.g., K-means, hierarchical clustering) and dimensionality reduction (e.g., PCA, t-SNE).\n",
       "\n",
       "3. **Model Selection**: Scikit-learn provides tools for model evaluation and selection, including cross-validation techniques and metrics to assess model performance (accuracy, precision, recall, etc.).\n",
       "\n",
       "4. **Preprocessing**: The library includes functions for data preprocessing, such as scaling features, encoding categorical variables, and handling missing values.\n",
       "\n",
       "5. **Pipelines**: You can create pipelines to streamline the process of applying transformations and fitting models, which helps ensure reproducibility and maintainability.\n",
       "\n",
       "6. **Integration with Other Libraries**: Scikit-learn works well with other scientific computing libraries in Python, such as NumPy, pandas, and Matplotlib.\n",
       "\n",
       "Here’s a simple example of using scikit-learn to perform a classification task:\n",
       "\n",
       "python\n",
       "import pandas as pd\n",
       "from sklearn.model_selection import train_test_split\n",
       "from sklearn.ensemble import RandomForestClassifier\n",
       "from sklearn.metrics import accuracy_score\n",
       "\n",
       "# Load a dataset (using Iris dataset as an example)\n",
       "from sklearn.datasets import load_iris\n",
       "data = load_iris()\n",
       "X = data.data\n",
       "y = data.target\n",
       "\n",
       "# Split the data into training and testing sets\n",
       "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
       "\n",
       "# Train a Random Forest Classifier\n",
       "model = RandomForestClassifier()\n",
       "model.fit(X_train, y_train)\n",
       "\n",
       "# Make predictions\n",
       "y_pred = model.predict(X_test)\n",
       "\n",
       "# Evaluate the model\n",
       "accuracy = accuracy_score(y_test, y_pred)\n",
       "print(f'Accuracy: {accuracy:.2f}')\n",
       "\n",
       "\n",
       "If you have specific questions about scikit-learn or need help with particular tasks or concepts, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "then how to do huff model using python\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Hierarchical Unified Feature Frequency (HUFF) model, commonly referred to as the HUFF model, is often associated with applications in natural language processing and data encoding due to its associative mapping of features and hierarchical organization. However, the terminology \"HUFF model\" could be somewhat ambiguous since it might refer to various concepts, including Huffman coding or Hierarchical models in general. \n",
       "\n",
       "If you're referring to a specific method (like Huffman coding, which is used for data compression), I can provide a Python implementation for that. Here's a simple example of how to implement Huffman coding in Python:\n",
       "\n",
       "### Huffman Coding Implementation in Python\n",
       "\n",
       "python\n",
       "import heapq\n",
       "from collections import defaultdict\n",
       "\n",
       "class Node:\n",
       "    def __init__(self, char, freq):\n",
       "        self.char = char\n",
       "        self.freq = freq\n",
       "        self.left = None\n",
       "        self.right = None\n",
       "\n",
       "    def __lt__(self, other):\n",
       "        return self.freq < other.freq\n",
       "\n",
       "def build_huffman_tree(text):\n",
       "    frequency = defaultdict(int)\n",
       "    \n",
       "    # Count frequency of each character\n",
       "    for char in text:\n",
       "        frequency[char] += 1\n",
       "\n",
       "    # Create a priority queue\n",
       "    priority_queue = [Node(char, freq) for char, freq in frequency.items()]\n",
       "    heapq.heapify(priority_queue)\n",
       "\n",
       "    # Merge nodes until there is only one node left\n",
       "    while len(priority_queue) > 1:\n",
       "        left = heapq.heappop(priority_queue)\n",
       "        right = heapq.heappop(priority_queue)\n",
       "        merged = Node(None, left.freq + right.freq)\n",
       "        merged.left = left\n",
       "        merged.right = right\n",
       "        heapq.heappush(priority_queue, merged)\n",
       "\n",
       "    return priority_queue[0]\n",
       "\n",
       "def build_codes(node, prefix=\"\", codebook={}):\n",
       "    if node:\n",
       "        if node.char is not None:\n",
       "            codebook[node.char] = prefix\n",
       "        build_codes(node.left, prefix + \"0\", codebook)\n",
       "        build_codes(node.right, prefix + \"1\", codebook)\n",
       "    return codebook\n",
       "\n",
       "def huffman_encoding(text):\n",
       "    root = build_huffman_tree(text)\n",
       "    huffman_codes = build_codes(root)\n",
       "    \n",
       "    encoded_output = ''.join(huffman_codes[char] for char in text)\n",
       "    \n",
       "    return encoded_output, huffman_codes\n",
       "\n",
       "def huffman_decoding(encoded_text, huffman_codes):\n",
       "    reverse_codes = {v: k for k, v in huffman_codes.items()}\n",
       "    current_code = \"\"\n",
       "    decoded_output = \"\"\n",
       "\n",
       "    for bit in encoded_text:\n",
       "        current_code += bit\n",
       "        if current_code in reverse_codes:\n",
       "            decoded_output += reverse_codes[current_code]\n",
       "            current_code = \"\"\n",
       "    \n",
       "    return decoded_output\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    text = \"this is an example for huffman encoding\"\n",
       "    print(\"Original Text: \", text)\n",
       "\n",
       "    encoded_text, huffman_codes = huffman_encoding(text)\n",
       "    print(\"Encoded Text: \", encoded_text)\n",
       "    print(\"Huffman Codes: \", huffman_codes)\n",
       "\n",
       "    decoded_text = huffman_decoding(encoded_text, huffman_codes)\n",
       "    print(\"Decoded Text: \", decoded_text)\n",
       "\n",
       "\n",
       "### Explanation:\n",
       "1. **Node Class**: Defines a node in the Huffman tree.\n",
       "2. **build_huffman_tree**: Creates a tree based on frequency of each character.\n",
       "3. **build_codes**: Generates binary codes for each character based on the tree structure.\n",
       "4. **huffman_encoding**: Encodes the input text into a binary representation.\n",
       "5. **huffman_decoding**: Decodes the binary representation back into the original text.\n",
       "\n",
       "### Usage:\n",
       "You can run the script, and it will encode the sample text and then decode it back to the original text.\n",
       "\n",
       "If this doesn't align with what you're asking for regarding \"HUFF model,\" please provide more context or details about the specific model or application you're referring to!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "WHAT IS HUFF MODEL\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Huffman Model, also known as the Huffman coding or Huffman encoding, is a method of source compression developed by David A. Huffman in 1952. It's a lossless data compression algorithm that assigns variable-length codes to symbols based on their frequency of occurrence in a text.\n",
       "\n",
       "**How it works**\n",
       "\n",
       "Given a stream of text with `n` distinct characters (e.g., alphabet), the Huffman model calculates the frequency of each character by counting its occurrences in the dataset. These frequencies are then used to build a binary tree where each internal node represents a symbol or two symbols combined into one (to be encoded later).\n",
       "\n",
       "Here's how it works:\n",
       "\n",
       "1. **Sort** the `n` distinct characters based on their frequencies, creating a priority queue.\n",
       "2. **Combine** adjacent elements with the smallest priorities in pairs to create an internal node while adding its code as the leftmost \"bit\" for the current pair.\n",
       "3. **Remove** the top two combinations and repeat steps 2-3 until there only remains single character codes left with no splitting.\n",
       "\n",
       "These single character codes are then encoded into a file when you try to decode, resulting in more bytes going out than what was used originally by compressing it (in other words: compression!).\n",
       "\n",
       "Here's an algorithm-level representation of how the Huffman model works using depth-first traversal for recursion:\n",
       "\n",
       "   -1. Sort frequency mapping `freq_map` based on its key to produce sorted list.\n",
       "\n",
       "   2. Initialize a new binary leaf node that serves as the root:\n",
       "\n",
       "        -(binaryLeafNode)\n",
       "\n",
       "           - assign bit length = size(node.left) + size(node.right)+1 \n",
       "\n",
       "   3. Create while loop, keep track of frequency \"i\" until it reaches zero. Here is a description for `freq_map[\"symbol\"], i` in python:\n",
       "\n",
       "     python\n",
       "for key in sorted(freq_map,reverse=True):\n",
       "    root=BinaryLeafNode(None)\n",
       "\n",
       "\n",
       "       (1) Make internal node to put each current bit of symbol in its left & right:\n",
       "        -(internal Node)\n",
       "\n",
       "         - assign the corresponding `left = sym_left()` & `right=sym_right()`, for each iteration you make nodes left & right like the algorithm. \n",
       "        -(2) Assign size as follows:\n",
       "        `(size(left)+1)+(size(right)+1)` \n",
       "      *(3) Make root node to connect it to internal node:-\n",
       "\n",
       "            -(Root)\n",
       "              - assign `left=internalNode` \n",
       "              - assign `right =binaryLeafNode`\n",
       "\n",
       "    (4) Use recursive case for each frequency until i reaches zero in the heap:\n",
       "\n",
       "     python\n",
       "    Node.addChild(node.left,BinaryLeafNode(symlabel))\n",
       "\n",
       "In your computer program, we can implement it like these:\n",
       "\n",
       " 1. Assign all the bits at leftmost and rightmost ends\n",
       "\n",
       "      -Python  \n",
       "     def make_nodes(self):\n",
       "       for i in self.code_dict.keys():\n",
       "        l = self.code_dict.get(i)[0]\n",
       "        r=self.code_dict.getleft(right)=i\n",
       "        root.left=BinaryLeafNode(r)\n",
       "\n",
       "\n",
       " 2. Iterate over `key list` to build these Binary Tree nodes as follows:\n",
       "      -python\n",
       "\n",
       "    def generate_codes(self):\n",
       "      current_node = Root()\n",
       "      \n",
       "      for key in sorted(self.code_dict.keys()):\n",
       "        node = BinaryLeafNode(key)\n",
       "        self.root = (node, )\n",
       "     \n",
       "   "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "is there any specific libraries to do this in python\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "To achieve various tasks related to natural language processing (NLP), machine learning, and deep learning with Python, several excellent libraries are available. Here are some of the most popular ones:\n",
       "\n",
       "**Natural Language Processing (NLP)**\n",
       "\n",
       "1. **NLTK (Natural Language Toolkit)**: A comprehensive library for NLP tasks such as tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
       "2. **spaCy**: A modern NLP library for performance-critical applications, offering high-performance, streamlined processing of text data.\n",
       "3. **Gensim**: A library for topic modeling and document similarity analysis.\n",
       "\n",
       "**Machine Learning**\n",
       "\n",
       "1. **scikit-learn**: A widely used library for machine learning tasks such as classification, regression, clustering, and dimensionality reduction.\n",
       "2. **TensorFlow**: An open-source machine learning framework developed by Google.\n",
       "3. **PyTorch**: An open-source machine learning framework developed by Facebook.\n",
       "\n",
       "**Deep Learning**\n",
       "\n",
       "1. **Keras**: A high-level neural networks API for deep learning tasks such as image classification, natural language processing, and time series analysis.\n",
       "2. **TensorFlow**: Also supports Keras syntax with its TensorFlow Python API.\n",
       "3. **PyTorch**: Supports both traditional Python APIs for PyTorch.\n",
       "\n",
       "**Other Libraries**\n",
       "\n",
       "1. **Pandas**: A powerful library for data manipulation and analysis.\n",
       "2. **NumPy**: A library for efficient numerical computation.\n",
       "3. **Matplotlib**: A popular library for data visualization.\n",
       "4. **Seaborn**: A library built on top of Matplotlib, providing a high-level interface for statistical graphics.\n",
       "\n",
       "**Large Language Models (LLMs)**\n",
       "\n",
       "For training and interacting with large language models like LLaMA, Chinchilla, or T5, you can use libraries such as:\n",
       "\n",
       "1. **Hugging Face Transformers**: A library that provides pre-trained models and easy-to-use APIs for fine-tuning and applying transformer-based models to various tasks.\n",
       "2. **PyTorch Transformers**: Supports the Transformers library, allowing you to train and apply large language models with PyTorch.\n",
       "\n",
       "These libraries are widely used in industry and academia, and learning them will help you tackle a wide range of NLP, machine learning, deep learning, and data science projects in Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "I mean to do huff model in python\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The Huffman coding algorithm is a popular technique used for compressing binary data. Here's a step-by-step guide on how to implement the Huffman coding algorithm in Python:\n",
       "\n",
       "**What is Huffman Coding?**\n",
       "\n",
       "Huffman coding is a variable-length prefix code that assigns shorter codes to more frequent characters in the input text. The algorithm works by building a binary tree, where each leaf node represents a character and its corresponding code.\n",
       "\n",
       "**Implementing Huffman Coding in Python**\n",
       "\n",
       "Here's an example implementation of the Huffman coding algorithm in Python:\n",
       "python\n",
       "import heapq\n",
       "\n",
       "class Node:\n",
       "    def __init__(self, char, freq):\n",
       "        self.char = char\n",
       "        self.freq = freq\n",
       "        self.left = None\n",
       "        self.right = None\n",
       "\n",
       "def calculate_frequency(data):\n",
       "    frequency = {}\n",
       "    for char in data:\n",
       "        if char not in frequency:\n",
       "            frequency[char] = 0\n",
       "        frequency[char] += 1\n",
       "    return frequency\n",
       "\n",
       "def build_huffman_tree(frequency):\n",
       "    heap = []\n",
       "    for char, freq in frequency.items():\n",
       "        node = Node(char, freq)\n",
       "        heapq.heappush(heap, (freq, node))\n",
       "\n",
       "    while len(heap) > 1:\n",
       "        lo, lo_node = heapq.heappop(heap)\n",
       "        hi, hi_node = heapq.heappop(heap)\n",
       "\n",
       "        merged = Node(None, lo + hi)\n",
       "        merged.left = lo_node\n",
       "        merged.right = hi_node\n",
       "\n",
       "        heapq.heappush(heap, (lo + hi, merged))\n",
       "\n",
       "    return heap[0]\n",
       "\n",
       "def build_codeword_tree(root):\n",
       "    codewords = {}\n",
       "    queue = [(root, \"\")]\n",
       "    while queue:\n",
       "        node, code = queue.pop(0)\n",
       "        if node.char is not None:\n",
       "            codewords[node.char] = code\n",
       "        else:\n",
       "            queue.append((node.left, code + \"0\"))\n",
       "            queue.append((node.right, code + \"1\"))\n",
       "    return codewords\n",
       "\n",
       "def huffman_encoding(data):\n",
       "    frequency = calculate_frequency(data)\n",
       "    huff_tree = build_huffman_tree(frequency)\n",
       "    codeword_dict = build_codeword_tree(huff_tree)\n",
       "\n",
       "    encoded_data = \"\"\n",
       "    for char in data:\n",
       "        encoded_char = \"\".join([codeword_dict[char_code] for char_code in bin(ord(char))[2:].zfill(8)])\n",
       "        encoded_data += encoded_char\n",
       "\n",
       "    return encoded_data, huff_tree, codeword_dict\n",
       "\n",
       "def huffman_decoding(encoded_data, huff_tree, codeword_dict):\n",
       "    decoded_data = \"\"\n",
       "    temp = list(huff_tree)\n",
       "    while len(temp) > 1:\n",
       "        for node in temp[0:2]:\n",
       "            if (node.char == None and\n",
       "                    encoded_data.startswith(codeword_dict[node.right]):\n",
       "                decoded_data += node.left.char\n",
       "                encoded_data = encoded_data[len(codeword_dict[node.right]):]\n",
       "                break\n",
       "            elif (node.char == None and\n",
       "                  encoded_data.startswith(codeword_dict[node.left])):\n",
       "                decoded_data += node.right.char\n",
       "                encoded_data = encoded_data[len(codeword_dict[node.left]):]\n",
       "                break\n",
       "\n",
       "    return decoded_data\n",
       "\n",
       "**How to use this implementation?**\n",
       "\n",
       "1. Convert the input data to lowercase using `data = \"\".join(sorted(set(data.lower()), key=data.lower.index))`\n",
       "2. Use `encoded_data, huff_tree, codeword_dict` returned from `huffman_encoding()`\n",
       "3. Use `huffman_decoding(encoded_data, huff_tree, codeword_dict)` to decode the encoded data\n",
       "\n",
       "**Example usage**\n",
       "python\n",
       "data = \"This is an example for huffman encoding\"\n",
       "encoded_data, huff_tree, codeword_dict = huffman_encoding(data)\n",
       "print(\"Encoded Data:\", encoded_data)\n",
       "\n",
       "decoded_data = huffman_decoding(encoded_data, huff_tree, codeword_dict)\n",
       "import matplotlib.pyplot as plt\n",
       "plt.text(0.45, 7.5, 'Huffman', rotation=-90, ha='center')\n",
       "plt.text(0.95, 8.4, f'Tree Length: {get_tree_length(huff_tree)}',rotation=0,ha='center' )\n",
       "plt.axis('off')\n",
       "plt.show()\n",
       "\n",
       "def get_tree_length(node):\n",
       "    if node.char is None:\n",
       "        if isinstance(node.left, str) and isinstance(node.right, str) :\n",
       "            return 1 +max(get_tree_length(node.left),get_tree_length(node.right))\n",
       "\n",
       "    else:\n",
       "       return 4+ max(get_tree_length(node.left), get_tree_length(node.right))\n",
       "\n",
       "You can install required python packages using pip\n",
       "bash\n",
       "pip install matplotlib\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a1a317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "what is huff model\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to figure out what the Huffman Model is. From what I know, Huffman coding is related to how we compress data efficiently, right? It has something to do with building a probability tree based on frequencies of characters in the data. The model itself isn't my strongest suit, but trying to piece it together might help.\n",
       "\n",
       "Hmm, so if the Huffman Model is part of the Huffman coding algorithm, maybe it's about creating that encoding scheme. But I've heard that in practice, especially with programming languages and systems, people often mean something else by \"Huffman model.\" Maybe they are talking about different kinds of Huffman models used in various machine learning tasks.\n",
       "\n",
       "Wait, that makes more sense. In deep learning and NLP, they might use variations of the Huffman coding algorithm. I think there's this concept called a \"probabilistic Huffman code.\" Let me see... No, wait, that's related to the Kraft-McMillan inequality for prefix codes. That's not exactly it.\n",
       "\n",
       "Maybe it's about model-based approaches in other areas like natural language processing or time series prediction. Oh, time series models too! I've heard of the \"autoregressive\" part and how certain models encode residuals. So if they call it a Huffman model in that context, maybe they're talking about using something similar to Huffman coding for modeling data.\n",
       "\n",
       "But there's more than one name for various machine learning techniques. How do we know which is which? Maybe in some cases people refer to their model as a Huffman tree where each code corresponds to an event. But I'm not sure how this ties back directly to the Huffman coding algorithm itself.\n",
       "\n",
       "Is \"Huffman model\" just a misnomer, and they're actually discussing different approaches like RNNs or LSTMs? Or perhaps they are referring specifically to hierarchical structures built using these models?\n",
       "\n",
       "I'm getting a bit confused. Let me start by recalling what the actual Huffman coding does. It builds a tree where each node represents the frequency of a symbol (or character). The symbols are then assigned codes based on their position in this tree, with left branches being 0 and right being 1. This results in an optimal prefix code that minimizes the expected code length.\n",
       "\n",
       "But in machine learning and other areas, they use similar mechanisms. For example, certain autoencoder models optimize layer weights to minimize residual entropy. The idea of treating each code assignment as a step that optimizes something sounds like Huffman coding's approach but applied to different aspects.\n",
       "\n",
       "Wait, now I'm thinking about the \"probabilistic automaton\" or other architectures where each code is built incrementally based on probabilities. Maybe these are seen as generalizations of the Huffman model in data modeling tasks.\n",
       "\n",
       "To sum up, while \"Huffman model\" isn't a concept I know offhand, there might be several contexts within machine learning and technical fields where they're referring to different models or approaches that use principles similar to Huffman coding. It's possible they're trying to describe hierarchical structure-building mechanisms using codes from these algorithms.\n",
       "\n",
       "I might have missed some nuances here, but this seems like a reasonable direction to explore further. Maybe I should look up terms related to \"probabilistic codes\" or \"hierarchical lossy models\" in machine learning and see if any papers or textbooks refer to them as Huffman models. That could give me more clarity on how they fit into broader research areas.\n",
       "</think>\n",
       "\n",
       "In many practical contexts, especially within machine learning and NLP, the term \"Huffman model\" is misinterpreted due to its similarity to Huffman coding, though it's applied in different ways. The Huffman coding algorithm is a method for lossless data compression by building a probability tree based on character frequencies. However, in machine learning, particularly with autoencoders and RNNs, similar unsupervised learning techniques are used.\n",
       "\n",
       "Key contexts where \"probabilistic codes\" might refer include:\n",
       "\n",
       "1. **Time Series Models**: In these models (e.g., LSTM autoencoders or ARIMA), the error from prediction is called residuals. The residual entropy can be optimized using \"lossy models,\" which sometimes resemble Huffman coding in their hierarchical structure-building approach.\n",
       "\n",
       "2. **Probabilistic Automata**: Hierarchical layer weights in architecture-like structures are used to optimize training, employing principles similar to how data reduction works in Huffman codes for compression and coding improvements.\n",
       "\n",
       "3. **Generalization of Huffman Coding**: In various machine learning tasks (e.g., KL-ARACNN), techniques that build hierarchical structure incrementally based on probability match the greedy steps of Huffman coding, focusing on model-based approaches rather than traditional prefix code trees.\n",
       "\n",
       "While \"Huffman model\" is a context error, related concepts in machine learning include probabilistic codes and hierarchical structures optimized using lossy modeling. These may describe mechanisms akin to Huffman codes but adapted for different data modeling tasks."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "I mean huff model in spatial context\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to understand the HUFF model from Wikipedia, but it's quite confusing. The article starts by comparing it to the typology used by Henry James, highlighting how different people approach understanding data differently—thinkers like those with degrees seeing patterns vs. idlers viewing chaos.\n",
       "\n",
       "Hmm, wait a minute, I thought Henry James was more of an existentialist writer focused on human existence and ideas about life. Maybe he developed this model conceptually later? Then why is it called the HUFF model? It seems arbitrary. Why three capital letters? Maybe they just took a few after his name to make it sound cooler or something.\n",
       "\n",
       "The article then distinguishes it from other models like Vygotsky's, which also deal with language development and cognitive growth. Maybe it relates to how different people perceive information—whether through observation or intellectual analysis. That makes sense because understanding data seems subjective depending on your background and methods.\n",
       "\n",
       "I'm trying to think of an example. Suppose a scientist sees complex patterns in data; they'd apply the HUFF approach with pencil and paper. An engineer might analyze large datasets using tools. A non-scientist, like a layperson, might rely more on visual observations without formal analysis. That makes sense because the model reflects varied ways humans handle information.\n",
       "\n",
       "But I'm not entirely sure about how to translate this into programming for Python. How would someone apply the HUFF approach in data processing? Maybe by automating patterns with scripts (观察 thinking), visualizing using libraries (可视化 tools), and interpreting results methodically, but it's pretty vague.\n",
       "\n",
       "I guess I need to break down both the typology from James and the HUFF model specifically. In programming terms, the typology might translate to specific Python modules or techniques for data analysis. For example, applying observation methods could involve visualization libraries like Plotly, while intellectual methods might involve automation or scripts using Selenium if dealing with websites.\n",
       "\n",
       "So, in a way, the HUFF model is a more general guide that can be applied through various programming practices depending on the context and tools people use. It's important because without a clear method, data analysis can feel chaotic instead of guided.\n",
       "\n",
       "I hope I'm making sense here. Maybe I'm missing some key points about how HUFF applies differently in Python compared to other methodologies. Perhaps it's more about adaptability in handling datasets, using multiple approaches for different tasks.\n",
       "</think>\n",
       "\n",
       "The concept inspired by Henry James’ typology is now referred to as the **HUFF model**, which offers a comparative framework for understanding data analysis through three key dimensions:\n",
       "\n",
       "1. **Observing**: This involves recognizing patterns and trends without formal intellectual methods. In programming terms, this might involve visualization tools like Plotly or libraries that aid in pattern recognition.\n",
       "\n",
       "2. **Visualizing**: By employing graphs and charts to represent data inherently, aiding in pattern identification. This aligns with the use of Python’s Matplotlib for plotting data.\n",
       "\n",
       "3. **Huffman (Focusing)**: Applying systematic methods for analysis, reducing chaos through meticulous processes. In programming contexts like using Selenium, automation tools can make tasks methodical.\n",
       "\n",
       "**Conclusion**: The HUFF model is more about adaptability than strict methodology, guiding humans towards organized approaches in data processing regardless of tools or programming practices."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "I mean huff model in spatial market analysis tool for retail and urban planning. do you know the details\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Alright, so I need to understand the H-U-F model for retail and urban planning. From what the assistant said earlier, it's a network-based model that looks at space and uses flow data from various sources like people searching for places or products in stores.\n",
       "\n",
       "First, I should break down each part of the model as per their contributions. The H-U-F model has several components: H (Human flows), U (User flows), F (Facility flows). Each contributes to how locations attract or are attracted by users.\n",
       "\n",
       "H stands for Human flows—probably where people go on foot and how often they visit different locations such as stores. U is User flows, so the number of people moving from a store to a location. This would show how much direct travel goes between places because of shops selling similar products.\n",
       "\n",
       "F represents facilitication flows—how good the facilities (like cleanliness, store location) are in making it easier for people to reach other stores and locations. So F shows the benefit of a place being in a certain area because it facilitates access to other places needed by users.\n",
       "\n",
       "Now, to simulate H-U-F models in Python, I need to model each component:\n",
       "\n",
       "H could use geospatial data like OpenStreetMap or Google Maps for street networks. It might also use flow data from real-world surveys where people reported their foot traffic into stores.\n",
       "\n",
       "U would involve modeling movement between store locations using factors like product similarity and distance. Maybe machine learning models, like decision trees, to predict which product is better matched with a store.\n",
       "\n",
       "F requires facility descriptions, but it's more about the benefits of being in certain locations. So I might use agent-based modeling where each user interacts with nearby facilitation units (like stores) based on proximity and accessibility costs.\n",
       "\n",
       "Putting them together, I'd build the H network as where users come from, model how they connect to U via their product types and distances, then have F facilitate this connection by encouraging nearby customers into reaching farther locations. This would create a dynamic network where people's choices both directly (H) link to physical locations (U), but also through social pressures (F) affect which places get more popularity or accessibility.\n",
       "\n",
       "I think I'll need to import packages like NetworkX for creating and manipulating networks, geopandas for handling geospatial data, openStreetMap or Google Maps APIs for street data. Also, maybe for some machine learning parts, using Scikit-learn or TensorFlow if needed for U and F components.\n",
       "\n",
       "A potential challenge could be integrating user flow with actual facility behavior dynamically without losing the network's structure. So I might use edge weights to represent how likely a connection is based on similarity and distance or encourage higher usage by increasing edge probabilities in H and connecting to users and F over time.\n",
       "\n",
       "Another thing: scalability. If more cities are considered, updating all models would mean rebuilding everything for each location. That could be resource-intensive but manageable by modularizing the components so each layer can run independently when the entire model is scaled up.\n",
       "\n",
       "I might need to test different parameters for H (like attraction strength based on store similarity), U (product features influencing flow), and F (facility quality affecting accessibility) to see how they interact. This could help refine predictions or validate existing models in new areas.\n",
       "\n",
       "Overall, it's a comprehensive approach that combines geospatial analysis with user behavior modeling, offering insights into spatial patterns and user decisions in retail and urban planning.\n",
       "</think>\n",
       "\n",
       "The H-U-F model is a network-based framework used for spatial market analysis, particularly relevant in retail and urban planning. Here's a structured breakdown of the components and their contributions:\n",
       "\n",
       "### Components of the H-U-F Model:\n",
       "1. **H (Human flows):**\n",
       "   - **Source:** Where people come from (e.g., stores, shops).\n",
       "   - **Purpose:** Models foot traffic data to identify patterns where users prefer to spend time in specific locations.\n",
       "   - **Data Used:**\n",
       "     - Foot Travel Surveys: Collecting real-world data on foot traffic into different retail and urban areas.\n",
       "\n",
       "2. **U (User flows):**\n",
       "   - **Source:** Who uses the locations?\n",
       "   - **Purpose:** Demonstrates direct movement between different store/zone pairs based on product similarity, distance, and convenience.\n",
       "   - **Data Used:**\n",
       "     - Product Similarity Data: Identifying which products are often purchased together (e.g., chocolates with gels).\n",
       "     - Distance Data: Measuring accessibility through time or geographic proximity.\n",
       "\n",
       "3. **F (Facility flows):**\n",
       "   - **Source:** How well facilities facilitate movement?\n",
       "   - **Purpose:** Reflects the benefits of being in a particular location, such as improved access to other facilities, ease of carrying out activities, hygiene practices, and cultural significance.\n",
       "   - **Data Used:**\n",
       "     - Accessibility Costs: Parameters influencing how easily someone moves between locations (e.g., parking, commuting).\n",
       "\n",
       "### Simulation Approach:\n",
       "1. **Modeling Human Flows (H):**\n",
       "   - Construct a spatial network where nodes represent stores and other zones, using geospatial data.\n",
       "   - Add foot traffic and product-similarity data to capture origin-destination patterns.\n",
       "\n",
       "2. **Modeling User Flows (U):**\n",
       "   - Build networks of location pairs showing product similarities based on foot travel data.\n",
       "   - Apply machine learning models to predict user flow directions with added randomness for diversity.\n",
       "\n",
       "3. **Modeling Facility Flows (F):**\n",
       "   - Incorporate factors like accessibility costs, shop facilities (e.g., cleanliness), and location attributes using agent-based modeling.\n",
       "   - Update probability of connection between locations moving users based on facility benefits.\n",
       "\n",
       "### Integration:\n",
       "- Model the H-U-F network to identify user movement that directly results from geography and product similarity (H) and dynamically attracts further usage based on social pressures (F).\n",
       "\n",
       "This integrated approach provides a comprehensive framework for understanding user decision-making in retail and urban planning, highlighting both direct geospatial effects and social influence mechanisms."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: \n",
      "do you know how to do this in python? is there any libraries to do this in python?\n",
      "\n",
      "Output: \n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to figure out the best way to build and analyze an LLM model using Python. From what I remember reading and maybe watching some tutorials, there are these big three tools: TensorFlow and its Keras package for deep learning, PyTorch for deep learning as well, and something called Ladder LLM or maybe a different framework like Vlad? Not sure about the latter.\n",
       "\n",
       "I think TensorFlow is pretty standard, but I've also heard people mention Keras. Keras is built on top of TensorFlow, right? So maybe if someone wants more flexibility, they can use Keras instead of TensorFlow directly. But then again, once you have Keras trained, how do you really take the model out for inference? That's where TensorFlow might be easier because it handles everything from training to serving.\n",
       "\n",
       "Another thought is about Hugging Face Transformers. I've heard a lot about this and think they have good models and tools for natural language processing tasks. They probably include things like model parameters that can be loaded using their PyTorch wrapper, making it easy to set up models without having to code everything from scratch. Plus, they must have some good documentation and example tutorials.\n",
       "\n",
       "I'm also curious about how these processes are connected. Like, using Hugging Face Transformers functions with Keras or TensorFlow for inference? What's the best way to do that?\n",
       "\n",
       "Wait, I think there might be different paths someone could take:\n",
       "\n",
       "1. Use TensorFlow directly for training and serving, maybe with Keras for model management.\n",
       "2. Jump into Hugging Face Transformers and leverage their modeling tools and frameworks.\n",
       "\n",
       "Each has a step up over the other, so it's like picking one path or perhaps using multiple tools together if needed.\n",
       "\n",
       "I'm not entirely sure about the limitations of TensorFlow compared to Hugging Face’s approach though. Maybe Hugging Face is better for some specific use cases but more flexible in overall framework design?\n",
       "\n",
       "Any other libraries I should consider? Oh, maybe something like PyTorch again since it's built on top of deep learning frameworks. Or perhaps frameworks outside of Python for NLP tasks, but those probably aren't as good.\n",
       "\n",
       "So in summary, the choices seem to be between TensorFlow/Keras+TensorFlow inference vs. using Hugging Face Transformers methods. Each has its pros and cons, so it might help to check out what the current trends are in LLM training and inference.\n",
       "</think>\n",
       "\n",
       "To effectively build and analyze an LLM model in Python, several approaches can be considered based on both tool availability and best practices for workflow optimization. Here's a structured summary of the options:\n",
       "\n",
       "### 1. TensorFlow/Keras + TensorFlow Inference\n",
       "- **Setting Up:**\n",
       "  - Use TensorFlow for training.\n",
       "  - Leverage Keras as a wrapper for TensorFlow when moving models to inference.\n",
       "- **Why It Works:**\n",
       "  - Integration with TensorFlow allows seamless use without needing additional packages.\n",
       "  - Useful when requiring explicit control over model architecture and customizations.\n",
       "\n",
       "### 2. Hugging Face Transformers\n",
       "- **Best Practices:**\n",
       "  - Use their models directly.\n",
       "  - Leverage their PyTorch framework for training and inference, enhancing flexibility due to pre-trained models.\n",
       "- **Why It Works:**\n",
       "  - Simplified setup with state-of-the-art models.\n",
       "  - Clear documentation, example tutorials, facilitating accessible experimentation.\n",
       "\n",
       "### 3. TensorFlow/Keras + Keras Inference\n",
       "- **Why It Works:**\n",
       "  - Combine ease of use from Keras with full inference via TensorFlow.\n",
       "  - Good for rapid prototyping and model management.\n",
       "\n",
       "### 4. PyTorch\n",
       "- **Best Practices:**\n",
       "  - Directly work with PyTorch for comprehensive control over deep learning.\n",
       "  - Leverage its powerful framework for model development.\n",
       "- **Why It Works:**\n",
       "  - Strong focus on GPU acceleration, suitable for high-performance tasks.\n",
       "  - Suitable if leveraging PyTorch's native libraries is preferred.\n",
       "\n",
       "### Considerations and Limitations:\n",
       "- **Efficiency Overlap:** TensorFlow/Keras are efficient over longer timescales, while Hugging Face Transformers may offer faster inference for specific workloads.\n",
       "- **Versatility:** Each approach addresses different needs with varying levels of abstraction, allowing flexibility in modeling strategy selection.\n",
       "\n",
       "### Conclusion\n",
       "The choice depends on the project's requirements. If you prioritize direct model management and flexibility, Keras + TensorFlow inference is a solid path. For quick prototyping and integration within frameworks like Hugging Face, leveraging their tools can streamline development and maintenance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "import openai\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "OLLAMA_API = 'http://localhost:11434/v1'\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"deepseek-r1:1.5b\"\n",
    "ollama_via_openai = OpenAI(base_url=OLLAMA_API, api_key='ollama')\n",
    "\n",
    "def ask_question(question):\n",
    "    system_prompt = \"You are a helpful technical tutor who answers questions about Python code, software engineering, data science, and LLMs.\"\n",
    "    user_prompt = question\n",
    "    print(f\"\\nInput: \\n{question}\")\n",
    "    print(f\"\\nOutput: \")\n",
    "    stream = ollama_via_openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
    "                  {\"role\": \"user\", \"content\": user_prompt}],\n",
    "        stream=True\n",
    "    )\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    \n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "        response = response.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        display_handle.update(Markdown(response))\n",
    "\n",
    "# Initial Question\n",
    "question = input(\"Enter your question: \")\n",
    "response = ask_question(question)\n",
    "\n",
    "# Follow-up loop\n",
    "while True:\n",
    "    follow_up = input(\"Ask a follow-up question (or press Enter to exit): \").strip()\n",
    "    if not follow_up:\n",
    "        break  # Exit the loop\n",
    "    response = ask_question(follow_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba59283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
